{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ed62d",
   "metadata": {},
   "source": [
    "# Test Suite: Bronze Layer Audit Logging Infrastructure\n",
    "\n",
    "**Purpose:** Validate the structure and configuration of `bronze.load_log` observability table\n",
    "\n",
    "**Scope:**\n",
    "- Extension requirements (pgcrypto for UUID generation)\n",
    "- Schema and table existence\n",
    "- Column definitions (names, types, nullability, defaults)\n",
    "- Primary key and sequence configuration\n",
    "- All 9 performance indexes\n",
    "- CHECK constraints (status, phase validation)\n",
    "- Data insertion and retrieval functionality\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Extension validation (pgcrypto availability)\n",
    "- Structural validation (11 columns with correct types)\n",
    "- Index coverage (9 indexes for query optimization)\n",
    "- Constraint enforcement (2 CHECK constraints for data quality)\n",
    "- Integration testing (INSERT, query, cleanup)\n",
    "- Default value verification (sequence, clock_timestamp)\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL server running\n",
    "- sql_retail_analytics_warehouse database exists\n",
    "- bronze schema exists (created by setup/create_schemas.sql)\n",
    "- `scripts/bronze/ddl_bronze_log.sql` has been executed\n",
    "- Connection credentials available\n",
    "- Required packages: psycopg2, pytest, ipytest, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf50e77",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies & Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46dfbf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pytest\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "\n",
    "# Configure ipytest for notebook usage\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'sql_retail_analytics_warehouse',\n",
    "    'user': 'postgres',\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'your_password_here')\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfdb7e0",
   "metadata": {},
   "source": [
    "## Fixture: Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b888bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixtures defined\n"
     ]
    }
   ],
   "source": [
    "@pytest.fixture(scope='module')\n",
    "def db_connection():\n",
    "    \"\"\"Create a database connection for tests.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def db_cursor(db_connection):\n",
    "    \"\"\"Create a cursor for executing queries.\"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "print(\"‚úÖ Fixtures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a1c3e",
   "metadata": {},
   "source": [
    "## Test Suite 1: Extension and Schema Validation\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_pgcrypto_extension_exists` - Validates pgcrypto extension is installed\n",
    "2. `test_bronze_schema_exists` - Validates bronze schema exists\n",
    "\n",
    "**How these tests work:**\n",
    "\n",
    "**Test 1: pgcrypto Extension Availability**\n",
    "- Queries `pg_extension` system catalog for 'pgcrypto' extension\n",
    "- ‚úÖ **Success:** COUNT(*) = 1 (extension installed and active)\n",
    "- ‚ùå **Failure:** COUNT(*) = 0 (extension missing, needs CREATE EXTENSION)\n",
    "- **Purpose:** Ensures UUID generation capability is available\n",
    "- **Why required:** `gen_random_uuid()` function used for generating batch run_id values\n",
    "- **When created:** DDL script creates with `CREATE EXTENSION IF NOT EXISTS pgcrypto`\n",
    "- **Privilege note:** Requires superuser or database owner to create extensions\n",
    "\n",
    "**Test 2: Bronze Schema Existence**\n",
    "- Queries `information_schema.schemata` for 'bronze' schema\n",
    "- ‚úÖ **Success:** COUNT(*) = 1 (schema exists)\n",
    "- ‚ùå **Failure:** COUNT(*) = 0 (schema missing, run setup/create_schemas.sql first)\n",
    "- **Purpose:** Validates medallion architecture bronze layer exists\n",
    "- **Prerequisite:** bronze schema should be created before running bronze DDL scripts\n",
    "\n",
    "**üîß Extension Background:**\n",
    "\n",
    "**pgcrypto Extension:**\n",
    "- **Purpose:** Provides cryptographic functions including UUID generation\n",
    "- **Key function:** `gen_random_uuid()` - generates version 4 (random) UUIDs\n",
    "- **Alternative:** PostgreSQL 13+ has built-in `gen_random_uuid()` without extension\n",
    "- **Backwards compatibility:** pgcrypto works on PostgreSQL 9.4+\n",
    "- **Use in bronze.load_log:** Every ETL batch gets unique run_id for grouping log entries\n",
    "\n",
    "**Why UUID over SERIAL for run_id:**\n",
    "- **Uniqueness:** Globally unique across databases, servers, time\n",
    "- **Distribution:** Random values prevent hot spots in indexes\n",
    "- **Mergeability:** Can combine logs from multiple environments without conflicts\n",
    "- **Security:** Non-sequential (doesn't leak batch count information)\n",
    "- **Standard:** RFC 4122 compliance for interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2295cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_pgcrypto_extension_exists \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 50%]\u001b[0m\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_bronze_schema_exists \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_pgcrypto_extension_exists(db_cursor):\n",
    "    \"\"\"Verify pgcrypto extension is installed (required for gen_random_uuid()).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM pg_extension \n",
    "        WHERE extname = 'pgcrypto'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"pgcrypto extension must be installed\"\n",
    "\n",
    "def test_bronze_schema_exists(db_cursor):\n",
    "    \"\"\"Verify bronze schema exists.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.schemata \n",
    "        WHERE schema_name = 'bronze'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"bronze schema must exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d6d3b",
   "metadata": {},
   "source": [
    "## Test Suite 2: Table Existence and Basic Structure\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_table_exists` - Validates bronze.load_log table exists\n",
    "2. `test_load_log_column_count` - Validates table has exactly 11 columns\n",
    "\n",
    "**How these tests work:**\n",
    "\n",
    "**Test 1: Table Existence**\n",
    "- Queries `information_schema.tables` for bronze.load_log\n",
    "- Filters by schema='bronze' AND table_name='load_log'\n",
    "- ‚úÖ **Success:** COUNT(*) = 1 (table exists and is unique)\n",
    "- ‚ùå **Failure:** COUNT(*) = 0 (table missing) or >1 (duplicate tables)\n",
    "- **Purpose:** Confirms DDL script executed successfully\n",
    "- **When created:** `CREATE TABLE IF NOT EXISTS bronze.load_log` in ddl_bronze_log.sql\n",
    "\n",
    "**Test 2: Column Count Validation**\n",
    "- Queries `information_schema.columns` for all columns in bronze.load_log\n",
    "- ‚úÖ **Success:** COUNT(*) = 11 (complete column set)\n",
    "- ‚ùå **Failure:** COUNT(*) ‚â† 11 (columns missing or extras added)\n",
    "- **Purpose:** Quick structural integrity check before detailed validation\n",
    "- **Complete column list:**\n",
    "  1. id (BIGSERIAL PRIMARY KEY)\n",
    "  2. run_id (UUID - batch identifier)\n",
    "  3. phase (TEXT - operation type)\n",
    "  4. table_name (TEXT - target table)\n",
    "  5. file_path (TEXT - source file)\n",
    "  6. status (TEXT - OK/ERROR)\n",
    "  7. rows_loaded (BIGINT - count)\n",
    "  8. started_at (TIMESTAMPTZ - start time)\n",
    "  9. finished_at (TIMESTAMPTZ - end time)\n",
    "  10. duration_sec (INTEGER - elapsed seconds)\n",
    "  11. message (TEXT - details/errors)\n",
    "\n",
    "**üìä bronze.load_log Purpose:**\n",
    "\n",
    "**Observability Requirements:**\n",
    "- **What:** Tracks every ETL operation across all bronze layer loads\n",
    "- **When:** Logs written during COPY operations from CSV to bronze tables\n",
    "- **Why:** Debugging, auditing, performance monitoring, compliance\n",
    "- **How:** Append-only log (never UPDATE or DELETE in production)\n",
    "\n",
    "**Log Entry Types:**\n",
    "- **START:** Batch begins (single entry per run_id)\n",
    "- **TRUNCATE:** Table truncated before load\n",
    "- **COPY:** CSV data loaded into table (logs rows_loaded)\n",
    "- **SEPARATOR:** Milestone marker between operations\n",
    "- **FINISH:** Batch completes successfully\n",
    "- **ERROR:** Operation failed (captures SQLERRM message)\n",
    "\n",
    "**Operational Benefits:**\n",
    "- **Debugging:** Identify which file/table caused failures\n",
    "- **Performance:** Track duration_sec to find slow operations\n",
    "- **Auditing:** Complete history of all data loads\n",
    "- **Monitoring:** Query for ERROR status to trigger alerts\n",
    "- **Compliance:** Immutable log for regulatory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738e1211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_table_exists \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 50%]\u001b[0m\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_column_count \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_table_exists(db_cursor):\n",
    "    \"\"\"Verify bronze.load_log table exists.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'bronze' \n",
    "          AND table_name = 'load_log'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"bronze.load_log table must exist\"\n",
    "\n",
    "def test_load_log_column_count(db_cursor):\n",
    "    \"\"\"Verify bronze.load_log has expected number of columns.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_schema = 'bronze' \n",
    "          AND table_name = 'load_log'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 11, \"bronze.load_log should have 11 columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd8fdd",
   "metadata": {},
   "source": [
    "## Test Suite 3: Column Definitions\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_column_definitions` - Validates all 11 columns with correct types and nullability\n",
    "2. `test_load_log_primary_key` - Validates id column is the primary key\n",
    "\n",
    "**How these tests work:**\n",
    "\n",
    "**Test 1: Column Definition Validation**\n",
    "- Queries `information_schema.columns` for all column metadata\n",
    "- Builds dictionary: `{column_name: {type, nullable}}` from query results\n",
    "- Compares against expected definitions for all 11 columns\n",
    "- ‚úÖ **Success:** All columns exist with correct types and nullability\n",
    "- ‚ùå **Failure:** Missing columns, type mismatches, or incorrect NULL constraints\n",
    "\n",
    "**Detailed Column Specifications:**\n",
    "\n",
    "**Identity & Grouping:**\n",
    "- **id** (bigint, NOT NULL)\n",
    "  - Auto-incrementing surrogate key via BIGSERIAL\n",
    "  - Primary key for unique row identification\n",
    "  - Supports up to 9.2 quintillion rows before wraparound\n",
    "  \n",
    "- **run_id** (uuid, NOT NULL)\n",
    "  - Groups all log entries for a single batch execution\n",
    "  - Generated via `gen_random_uuid()` at batch start\n",
    "  - All operations in same batch share same run_id\n",
    "  - Enables batch-level queries and analysis\n",
    "\n",
    "**Operation Metadata:**\n",
    "- **phase** (text, NOT NULL)\n",
    "  - Identifies operation type: START, TRUNCATE, COPY, SEPARATOR, FINISH, ERROR\n",
    "  - CHECK constraint enforces valid values\n",
    "  - Used for filtering logs by operation type\n",
    "  \n",
    "- **table_name** (text, nullable)\n",
    "  - Schema-qualified table name (e.g., 'bronze.erp_cust_az12')\n",
    "  - NULL for batch-level events (START, FINISH)\n",
    "  - Populated for table-specific operations (TRUNCATE, COPY)\n",
    "  \n",
    "- **file_path** (text, nullable)\n",
    "  - Absolute path to source CSV file (server-side path)\n",
    "  - NULL for non-file operations (TRUNCATE, SEPARATOR)\n",
    "  - Critical for data lineage and reloading\n",
    "\n",
    "**Status & Metrics:**\n",
    "- **status** (text, NOT NULL)\n",
    "  - Operation outcome: 'OK' or 'ERROR'\n",
    "  - CHECK constraint enforces valid values\n",
    "  - Primary field for error detection queries\n",
    "  \n",
    "- **rows_loaded** (bigint, nullable)\n",
    "  - Number of rows successfully copied\n",
    "  - NULL for non-COPY operations\n",
    "  - Validates expected row counts from source files\n",
    "\n",
    "**Timing:**\n",
    "- **started_at** (timestamp with time zone, NOT NULL)\n",
    "  - Operation start timestamp with timezone awareness\n",
    "  - Defaults to `clock_timestamp()` (actual clock time, not transaction time)\n",
    "  - Used for time-range queries and chronological ordering\n",
    "  \n",
    "- **finished_at** (timestamp with time zone, nullable)\n",
    "  - Operation completion timestamp\n",
    "  - NULL for in-progress or failed operations\n",
    "  - Used to calculate duration_sec\n",
    "  \n",
    "- **duration_sec** (integer, nullable)\n",
    "  - Elapsed time in seconds (finished_at - started_at)\n",
    "  - NULL if operation incomplete\n",
    "  - Performance analysis and slow operation detection\n",
    "\n",
    "**Details:**\n",
    "- **message** (text, nullable)\n",
    "  - Free-form descriptive text for successful operations\n",
    "  - PostgreSQL SQLERRM content for failures\n",
    "  - Human-readable context for debugging\n",
    "\n",
    "**Test 2: Primary Key Validation**\n",
    "- Queries `pg_index` and `pg_attribute` for primary key columns\n",
    "- ‚úÖ **Success:** Primary key is exactly 'id' column\n",
    "- ‚ùå **Failure:** No PK, wrong column, or composite PK\n",
    "- **Purpose:** Ensures unique row identification and prevents duplicates\n",
    "\n",
    "**üéØ Design Rationale:**\n",
    "\n",
    "**TEXT vs VARCHAR:**\n",
    "- All string columns use TEXT (unbounded length)\n",
    "- No performance difference in PostgreSQL\n",
    "- Avoids arbitrary length limits\n",
    "- Simpler schema (no need to specify lengths)\n",
    "\n",
    "**TIMESTAMPTZ over TIMESTAMP:**\n",
    "- Timezone-aware timestamps prevent ambiguity\n",
    "- Handles daylight saving time correctly\n",
    "- Stores in UTC, displays in session timezone\n",
    "- Critical for distributed systems\n",
    "\n",
    "**BIGINT for row counts:**\n",
    "- Supports tables with billions of rows\n",
    "- Future-proof for data growth\n",
    "- INTEGER would limit to 2.1 billion rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59335aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_column_definitions \u001b[32mPASSED\u001b[0m\u001b[32m               [ 50%]\u001b[0m\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_primary_key \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.20s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_column_definitions(db_cursor):\n",
    "    \"\"\"Verify all required columns exist with correct data types.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            is_nullable\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'bronze'\n",
    "          AND table_name = 'load_log'\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    \n",
    "    columns = db_cursor.fetchall()\n",
    "    column_dict = {col[0]: {'type': col[1], 'nullable': col[2]} for col in columns}\n",
    "    \n",
    "    # Expected column definitions\n",
    "    expected_columns = {\n",
    "        'id': {'type': 'bigint', 'nullable': 'NO'},\n",
    "        'run_id': {'type': 'uuid', 'nullable': 'NO'},\n",
    "        'phase': {'type': 'text', 'nullable': 'NO'},\n",
    "        'table_name': {'type': 'text', 'nullable': 'YES'},\n",
    "        'file_path': {'type': 'text', 'nullable': 'YES'},\n",
    "        'status': {'type': 'text', 'nullable': 'NO'},\n",
    "        'rows_loaded': {'type': 'bigint', 'nullable': 'YES'},\n",
    "        'started_at': {'type': 'timestamp with time zone', 'nullable': 'NO'},\n",
    "        'finished_at': {'type': 'timestamp with time zone', 'nullable': 'YES'},\n",
    "        'duration_sec': {'type': 'integer', 'nullable': 'YES'},\n",
    "        'message': {'type': 'text', 'nullable': 'YES'}\n",
    "    }\n",
    "    \n",
    "    # Verify each expected column\n",
    "    for col_name, expected in expected_columns.items():\n",
    "        assert col_name in column_dict, f\"Column '{col_name}' must exist\"\n",
    "        assert column_dict[col_name]['type'] == expected['type'], \\\n",
    "            f\"Column '{col_name}' should be {expected['type']}, got {column_dict[col_name]['type']}\"\n",
    "        assert column_dict[col_name]['nullable'] == expected['nullable'], \\\n",
    "            f\"Column '{col_name}' nullable mismatch\"\n",
    "\n",
    "def test_load_log_primary_key(db_cursor):\n",
    "    \"\"\"Verify id column is the primary key.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT a.attname\n",
    "        FROM pg_index i\n",
    "        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n",
    "        WHERE i.indrelid = 'bronze.load_log'::regclass\n",
    "          AND i.indisprimary\n",
    "    \"\"\")\n",
    "    pk_columns = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert pk_columns == ['id'], \"Primary key should be 'id' column only\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82e12a",
   "metadata": {},
   "source": [
    "## Test Suite 4: Index Validation\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_indexes_exist` - Validates all 9 indexes are created correctly\n",
    "\n",
    "**How this test works:**\n",
    "\n",
    "**Index Coverage Validation:**\n",
    "- Queries `pg_indexes` system catalog for all indexes on bronze.load_log\n",
    "- ‚úÖ **Success:** All 9 expected indexes exist (plus 1 automatic PK index = 10 total)\n",
    "- ‚ùå **Failure:** Missing indexes or unexpected index count\n",
    "- **Purpose:** Ensures query performance optimization infrastructure is in place\n",
    "\n",
    "**Expected Indexes (9 total):**\n",
    "\n",
    "1. **idx_load_log_run_id** - Index on `run_id` column\n",
    "   - **Query pattern:** Retrieve all log entries for a specific batch\n",
    "   - **Use case:** `WHERE run_id = '...'` - batch debugging and analysis\n",
    "   - **Cardinality:** High (one UUID per batch execution)\n",
    "   \n",
    "2. **idx_load_log_phase** - Index on `phase` column\n",
    "   - **Query pattern:** Filter by operation type\n",
    "   - **Use case:** `WHERE phase = 'ERROR'` - find all failures\n",
    "   - **Cardinality:** Low (7 distinct values: START, TRUNCATE, COPY, SEPARATOR, FINISH, ERROR)\n",
    "   \n",
    "3. **idx_load_log_table_name** - Index on `table_name` column\n",
    "   - **Query pattern:** Per-table load history\n",
    "   - **Use case:** `WHERE table_name = 'bronze.erp_cust_az12'` - table-specific drilldowns\n",
    "   - **Cardinality:** Medium (one per bronze table)\n",
    "   \n",
    "4. **idx_load_log_file_path** - Index on `file_path` column\n",
    "   - **Query pattern:** Source file provenance and lineage\n",
    "   - **Use case:** `WHERE file_path LIKE '%cust_info.csv'` - track file reloads\n",
    "   - **Cardinality:** High (varies with data refresh frequency)\n",
    "   \n",
    "5. **idx_load_log_status** - Index on `status` column\n",
    "   - **Query pattern:** Error detection and health monitoring\n",
    "   - **Use case:** `WHERE status = 'ERROR'` - quick error scans for alerting\n",
    "   - **Cardinality:** Very low (2 values: OK, ERROR)\n",
    "   \n",
    "6. **idx_load_log_rows_loaded** - Index on `rows_loaded` column\n",
    "   - **Query pattern:** Volume analysis and anomaly detection\n",
    "   - **Use case:** `WHERE rows_loaded > 1000000` - heavy vs light loads\n",
    "   - **Cardinality:** High (continuous numeric values)\n",
    "   \n",
    "7. **idx_load_log_started_at** - Index on `started_at` column\n",
    "   - **Query pattern:** Time-range queries and chronological analysis\n",
    "   - **Use case:** `WHERE started_at >= '2025-01-01'` - monthly/daily reports\n",
    "   - **Cardinality:** Very high (timestamp precision to microseconds)\n",
    "   \n",
    "8. **idx_load_log_finished_at** - Index on `finished_at` column\n",
    "   - **Query pattern:** Completion time analysis\n",
    "   - **Use case:** `WHERE finished_at BETWEEN ... AND ...` - SLA monitoring\n",
    "   - **Cardinality:** Very high (timestamp precision)\n",
    "   \n",
    "9. **idx_load_log_duration** - Index on `duration_sec` column\n",
    "   - **Query pattern:** Performance profiling and optimization\n",
    "   - **Use case:** `WHERE duration_sec > 300 ORDER BY duration_sec DESC` - slow operations\n",
    "   - **Cardinality:** Medium (seconds precision, limited range)\n",
    "\n",
    "**Plus Primary Key Index:**\n",
    "- Automatically created on `id` column\n",
    "- Usually named `load_log_pkey`\n",
    "- B-tree index for unique constraint enforcement\n",
    "\n",
    "**üìà Index Strategy Rationale:**\n",
    "\n",
    "**Why 9 Indexes on an Audit Table?**\n",
    "\n",
    "**Query Patterns Supported:**\n",
    "- **Batch analysis:** run_id index groups all operations for a batch\n",
    "- **Error monitoring:** status index enables fast `WHERE status = 'ERROR'` scans\n",
    "- **Performance profiling:** duration_sec index finds slow operations\n",
    "- **Table-specific history:** table_name index for per-table debugging\n",
    "- **Time-series analysis:** started_at/finished_at for trend analysis\n",
    "\n",
    "**Performance vs Storage Trade-offs:**\n",
    "- **Write cost:** Indexes slow INSERT operations (acceptable for low-volume logs)\n",
    "- **Storage cost:** ~10-20% overhead per index (worth it for query speed)\n",
    "- **Read benefit:** 100-1000x faster queries for common patterns\n",
    "- **Append-only pattern:** No UPDATE/DELETE means no index maintenance overhead\n",
    "\n",
    "**Why NOT Index message Column:**\n",
    "- Free-form text (high cardinality, low query value)\n",
    "- Full-text search would require GIN index (expensive)\n",
    "- Typically queried after narrowing by other columns\n",
    "\n",
    "**Monitoring Queries Enabled:**\n",
    "```sql\n",
    "-- Find recent errors (status + started_at indexes)\n",
    "WHERE status = 'ERROR' AND started_at > NOW() - INTERVAL '1 day'\n",
    "\n",
    "-- Slow operations by table (table_name + duration_sec indexes)\n",
    "WHERE table_name = 'bronze.crm_sales_details' AND duration_sec > 60\n",
    "\n",
    "-- Batch timeline (run_id + started_at indexes)\n",
    "WHERE run_id = '...' ORDER BY started_at\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafae760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_indexes_exist \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_indexes_exist(db_cursor):\n",
    "    \"\"\"Verify all expected indexes are created.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT indexname\n",
    "        FROM pg_indexes\n",
    "        WHERE schemaname = 'bronze'\n",
    "          AND tablename = 'load_log'\n",
    "        ORDER BY indexname\n",
    "    \"\"\")\n",
    "    \n",
    "    indexes = [row[0] for row in db_cursor.fetchall()]\n",
    "    \n",
    "    # Expected indexes (excluding primary key index)\n",
    "    expected_indexes = [\n",
    "        'idx_load_log_duration',\n",
    "        'idx_load_log_file_path',\n",
    "        'idx_load_log_finished_at',\n",
    "        'idx_load_log_phase',\n",
    "        'idx_load_log_rows_loaded',\n",
    "        'idx_load_log_run_id',\n",
    "        'idx_load_log_started_at',\n",
    "        'idx_load_log_status',\n",
    "        'idx_load_log_table_name'\n",
    "    ]\n",
    "    \n",
    "    for idx_name in expected_indexes:\n",
    "        assert idx_name in indexes, f\"Index '{idx_name}' must exist\"\n",
    "    \n",
    "    # Verify we have at least 10 indexes (9 explicit + 1 PK)\n",
    "    assert len(indexes) >= 10, f\"Expected at least 10 indexes, found {len(indexes)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea970f",
   "metadata": {},
   "source": [
    "## Test Suite 5: Constraint Validation\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_check_constraints` - Validates CHECK constraints on status and phase columns\n",
    "\n",
    "**How this test works:**\n",
    "\n",
    "**CHECK Constraint Validation:**\n",
    "- Queries `pg_constraint` system catalog for CHECK constraints\n",
    "- Uses `pg_get_constraintdef()` to retrieve constraint definitions\n",
    "- ‚úÖ **Success:** Both CHECK constraints exist with correct value lists\n",
    "- ‚ùå **Failure:** Constraints missing or incomplete value definitions\n",
    "- **Purpose:** Ensures data quality rules are enforced at database level\n",
    "\n",
    "**Constraint 1: status CHECK Constraint**\n",
    "\n",
    "**Constraint name:** `load_log_status_chk`\n",
    "\n",
    "**Definition:** `CHECK (status IN ('OK', 'ERROR'))`\n",
    "\n",
    "**Allowed values:**\n",
    "- **'OK'** - Operation completed successfully\n",
    "- **'ERROR'** - Operation failed (error details in message column)\n",
    "\n",
    "**Why only 2 values:**\n",
    "- Binary outcome model: success or failure\n",
    "- No intermediate states (operations are atomic)\n",
    "- KISS principle: simple status model is easier to query and monitor\n",
    "\n",
    "**Creation note:** `NOT VALID` on creation\n",
    "- Constraint is defined but not immediately validated against existing data\n",
    "- Allows fast DDL execution (no table scan)\n",
    "- Future rows are validated; old rows can be fixed lazily\n",
    "- Can validate later with: `ALTER TABLE ... VALIDATE CONSTRAINT load_log_status_chk`\n",
    "\n",
    "**Constraint 2: phase CHECK Constraint**\n",
    "\n",
    "**Constraint name:** `load_log_phase_chk`\n",
    "\n",
    "**Definition:** `CHECK (phase IN ('START','TRUNCATE','COPY','SEPARATOR','FINISH','ERROR'))`\n",
    "\n",
    "**Allowed values:**\n",
    "- **'START'** - Batch execution begins (first log entry for run_id)\n",
    "- **'TRUNCATE'** - Table truncated before loading\n",
    "- **'COPY'** - Data copied from CSV to table (primary operation)\n",
    "- **SEPARATOR'** - Milestone marker between logical sections\n",
    "- **'FINISH'** - Batch execution completes successfully (last log entry)\n",
    "- **'ERROR'** - Critical failure (batch aborted)\n",
    "\n",
    "**Phase Flow:**\n",
    "```\n",
    "Normal batch: START ‚Üí TRUNCATE ‚Üí COPY ‚Üí COPY ‚Üí ... ‚Üí FINISH\n",
    "Failed batch: START ‚Üí TRUNCATE ‚Üí COPY ‚Üí ERROR\n",
    "```\n",
    "\n",
    "**Why these phases:**\n",
    "- **START:** Marks batch boundary, generates run_id\n",
    "- **TRUNCATE:** Documents destructive operation (important for auditing)\n",
    "- **COPY:** Core data load operation (most frequent phase)\n",
    "- **SEPARATOR:** Visual grouping in logs (optional, organizational)\n",
    "- **FINISH:** Confirms successful completion\n",
    "- **ERROR:** Captures failure point for debugging\n",
    "\n",
    "**Creation note:** `NOT VALID` on creation (same rationale as status constraint)\n",
    "\n",
    "**üîí CHECK Constraint Benefits:**\n",
    "\n",
    "**Data Quality Enforcement:**\n",
    "- **Prevents typos:** Can't insert `phase = 'COYP'` (typo) or `status = 'FAIL'` (wrong term)\n",
    "- **Schema documentation:** Constraint lists valid values in database metadata\n",
    "- **Query optimization:** PostgreSQL can eliminate impossible conditions\n",
    "- **Type safety:** Complements TEXT type with controlled vocabulary\n",
    "\n",
    "**NOT VALID Strategy:**\n",
    "- **Fast deployments:** No table scan on constraint creation\n",
    "- **Flexible validation:** Can fix existing data before validating\n",
    "- **Safe evolution:** Add new phases without breaking old data\n",
    "- **Production-friendly:** Zero downtime for constraint addition\n",
    "\n",
    "**Why NOT Use ENUM Type:**\n",
    "- **Flexibility:** Adding phase values requires ALTER TYPE (locks table)\n",
    "- **Text compatibility:** No casting needed in queries\n",
    "- **Cross-database:** TEXT + CHECK is more portable\n",
    "- **Simplicity:** Easier to modify constraint than enum definition\n",
    "\n",
    "**Validation Queries Enabled:**\n",
    "```sql\n",
    "-- All errors (status constraint ensures only valid values)\n",
    "SELECT * FROM bronze.load_log WHERE status = 'ERROR'\n",
    "\n",
    "-- Find batch failures (phase constraint ensures valid phase names)\n",
    "SELECT * FROM bronze.load_log WHERE phase = 'ERROR'\n",
    "\n",
    "-- Invalid queries caught at INSERT time\n",
    "INSERT INTO bronze.load_log (run_id, phase, status) \n",
    "VALUES (gen_random_uuid(), 'INVALID', 'OK')  -- ‚ùå Fails: phase check\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81f4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_check_constraints \u001b[32mPASSED\u001b[0m\u001b[32m                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_check_constraints(db_cursor):\n",
    "    \"\"\"Verify CHECK constraints on status and phase columns.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT conname, pg_get_constraintdef(oid) AS definition\n",
    "        FROM pg_constraint\n",
    "        WHERE conrelid = 'bronze.load_log'::regclass\n",
    "          AND contype = 'c'\n",
    "        ORDER BY conname\n",
    "    \"\"\")\n",
    "    \n",
    "    constraints = {row[0]: row[1] for row in db_cursor.fetchall()}\n",
    "    \n",
    "    # Verify status constraint\n",
    "    assert 'load_log_status_chk' in constraints, \"status CHECK constraint must exist\"\n",
    "    assert 'OK' in constraints['load_log_status_chk'], \"status constraint should include 'OK'\"\n",
    "    assert 'ERROR' in constraints['load_log_status_chk'], \"status constraint should include 'ERROR'\"\n",
    "    \n",
    "    # Verify phase constraint\n",
    "    assert 'load_log_phase_chk' in constraints, \"phase CHECK constraint must exist\"\n",
    "    phases = ['START', 'TRUNCATE', 'COPY', 'SEPARATOR', 'FINISH', 'ERROR']\n",
    "    for phase in phases:\n",
    "        assert phase in constraints['load_log_phase_chk'], \\\n",
    "            f\"phase constraint should include '{phase}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b9ab9",
   "metadata": {},
   "source": [
    "## Test Suite 6: Default Values and Sequences\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_id_sequence` - Validates id column auto-increment via sequence\n",
    "2. `test_load_log_started_at_default` - Validates started_at default to clock_timestamp()\n",
    "\n",
    "**How these tests work:**\n",
    "\n",
    "**Test 1: Auto-Increment Sequence**\n",
    "- Queries `information_schema.columns` for id column's default value\n",
    "- ‚úÖ **Success:** Default contains 'nextval' (sequence-based auto-increment)\n",
    "- ‚ùå **Failure:** No default or non-sequence default\n",
    "- **Purpose:** Confirms BIGSERIAL creates sequence for primary key generation\n",
    "\n",
    "**BIGSERIAL Mechanics:**\n",
    "\n",
    "**What BIGSERIAL Does:**\n",
    "```sql\n",
    "-- BIGSERIAL is syntactic sugar for:\n",
    "id BIGINT NOT NULL DEFAULT nextval('bronze.load_log_id_seq')\n",
    "-- Plus: CREATE SEQUENCE bronze.load_log_id_seq OWNED BY bronze.load_log.id\n",
    "```\n",
    "\n",
    "**Sequence Properties:**\n",
    "- **Name:** Typically `tablename_columnname_seq` (e.g., `load_log_id_seq`)\n",
    "- **Range:** 1 to 9,223,372,036,854,775,807 (9.2 quintillion)\n",
    "- **Auto-increment:** Each INSERT gets next value automatically\n",
    "- **Ownership:** Sequence is dropped if column is dropped\n",
    "- **Cache:** PostgreSQL caches 1 value by default (configurable)\n",
    "\n",
    "**Why BIGSERIAL over SERIAL:**\n",
    "- **SERIAL:** INTEGER range (2.1 billion max)\n",
    "- **BIGSERIAL:** BIGINT range (9.2 quintillion max)\n",
    "- **Audit table growth:** Can accumulate billions of log entries over years\n",
    "- **No wraparound concerns:** Effectively unlimited for any realistic workload\n",
    "- **Minimal overhead:** BIGINT is 8 bytes vs INTEGER's 4 bytes (negligible)\n",
    "\n",
    "**Test 2: Timestamp Default Validation**\n",
    "- Queries `information_schema.columns` for started_at default value\n",
    "- ‚úÖ **Success:** Default contains 'clock_timestamp'\n",
    "- ‚ùå **Failure:** No default or wrong function\n",
    "- **Purpose:** Ensures operation start time is captured automatically\n",
    "\n",
    "**clock_timestamp() vs now() vs CURRENT_TIMESTAMP:**\n",
    "\n",
    "**clock_timestamp():**\n",
    "- **Returns:** Actual current time when function is called\n",
    "- **Changes:** During transaction execution (real-time clock)\n",
    "- **Use case:** Measure elapsed time within a transaction\n",
    "- **Example:** Multiple calls in same transaction return different times\n",
    "\n",
    "**now() / CURRENT_TIMESTAMP:**\n",
    "- **Returns:** Transaction start time (frozen)\n",
    "- **Changes:** Same value throughout entire transaction\n",
    "- **Use case:** Consistent timestamp across all statements in transaction\n",
    "- **Example:** All rows in same transaction get identical timestamp\n",
    "\n",
    "**Why clock_timestamp() for started_at:**\n",
    "```sql\n",
    "-- Example showing the difference:\n",
    "BEGIN;\n",
    "  SELECT now();              -- Returns: 2025-10-30 10:00:00\n",
    "  SELECT pg_sleep(5);        -- Wait 5 seconds\n",
    "  SELECT now();              -- Returns: 2025-10-30 10:00:00 (same!)\n",
    "  SELECT clock_timestamp();  -- Returns: 2025-10-30 10:00:05 (updated!)\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "**Audit Log Requirements:**\n",
    "- Need to measure actual elapsed time between operations\n",
    "- Each log entry should capture real-world time, not transaction time\n",
    "- duration_sec calculation requires real timestamps\n",
    "- Batch may have multiple operations; each needs distinct timestamp\n",
    "\n",
    "**üïê Timing Precision Benefits:**\n",
    "\n",
    "**Accurate Duration Calculation:**\n",
    "```sql\n",
    "-- With clock_timestamp():\n",
    "duration_sec = EXTRACT(EPOCH FROM (finished_at - started_at))\n",
    "-- Accurate to the microsecond\n",
    "\n",
    "-- With now():\n",
    "duration_sec = 0  -- All operations in same transaction show 0 duration!\n",
    "```\n",
    "\n",
    "**Performance Profiling:**\n",
    "- Identify slow COPY operations (large files)\n",
    "- Compare truncate vs copy durations\n",
    "- Find bottlenecks in ETL pipeline\n",
    "- Track improvements over time\n",
    "\n",
    "**Real-World Scenarios:**\n",
    "```sql\n",
    "-- Batch with 3 table loads\n",
    "START     - started_at: 10:00:00.000\n",
    "TRUNCATE  - started_at: 10:00:00.015  (15ms after start)\n",
    "COPY      - started_at: 10:00:00.030  (duration: 15ms)\n",
    "TRUNCATE  - started_at: 10:00:05.500  (5.5 seconds later)\n",
    "COPY      - started_at: 10:00:05.520  (duration: 20ms)\n",
    "FINISH    - started_at: 10:00:10.800  (total: 10.8 seconds)\n",
    "```\n",
    "\n",
    "**Timezone Awareness:**\n",
    "- TIMESTAMPTZ stores in UTC internally\n",
    "- Displays in session's timezone\n",
    "- Handles daylight saving time correctly\n",
    "- Critical for distributed systems and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436affec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_id_sequence \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 50%]\u001b[0m\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_started_at_default \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.20s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_id_sequence(db_cursor):\n",
    "    \"\"\"Verify id column uses a sequence (BIGSERIAL).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT column_default\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'bronze'\n",
    "          AND table_name = 'load_log'\n",
    "          AND column_name = 'id'\n",
    "    \"\"\")\n",
    "    \n",
    "    default_value = db_cursor.fetchone()[0]\n",
    "    assert default_value is not None, \"id column should have a default value\"\n",
    "    assert 'nextval' in default_value, \"id should use a sequence\"\n",
    "\n",
    "def test_load_log_started_at_default(db_cursor):\n",
    "    \"\"\"Verify started_at has clock_timestamp() default.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT column_default\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'bronze'\n",
    "          AND table_name = 'load_log'\n",
    "          AND column_name = 'started_at'\n",
    "    \"\"\")\n",
    "    \n",
    "    default_value = db_cursor.fetchone()[0]\n",
    "    assert default_value is not None, \"started_at should have a default value\"\n",
    "    assert 'clock_timestamp' in default_value.lower(), \"started_at should default to clock_timestamp()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc0cc0",
   "metadata": {},
   "source": [
    "## Test Suite 7: Integration Test - Insert and Verify\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_load_log_insert_and_query` - End-to-end test of INSERT, query, and cleanup\n",
    "\n",
    "**How this test works:**\n",
    "\n",
    "**Full Integration Workflow:**\n",
    "\n",
    "**Step 1: UUID Adapter Registration**\n",
    "```python\n",
    "from psycopg2.extras import register_uuid\n",
    "register_uuid()\n",
    "```\n",
    "- Registers PostgreSQL UUID type adapter with psycopg2\n",
    "- Enables passing Python `uuid.UUID` objects directly in queries\n",
    "- Without this: `ProgrammingError: can't adapt type 'UUID'`\n",
    "- Alternative: Convert to string `str(test_run_id)` but loses type safety\n",
    "\n",
    "**Step 2: Generate Unique run_id**\n",
    "```python\n",
    "test_run_id = uuid.uuid4()\n",
    "```\n",
    "- Creates version 4 (random) UUID\n",
    "- Format: `'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'` (32 hex digits + 4 hyphens)\n",
    "- Uniqueness: 2^122 possible values (1 in 5.3 undecillion)\n",
    "- Matches type of run_id column in bronze.load_log\n",
    "\n",
    "**Step 3: Insert Test Record**\n",
    "```sql\n",
    "INSERT INTO bronze.load_log (run_id, phase, status, message) \n",
    "VALUES (%s, 'START', 'OK', 'Test run from pytest')\n",
    "RETURNING id, run_id, phase, status\n",
    "```\n",
    "- Inserts minimal valid log entry (only NOT NULL columns)\n",
    "- Relies on defaults:\n",
    "  - `id`: Auto-generated via sequence (BIGSERIAL)\n",
    "  - `started_at`: Auto-populated via clock_timestamp()\n",
    "- `RETURNING` clause captures inserted values for verification\n",
    "- Parameterized query (safe from SQL injection)\n",
    "\n",
    "**Step 4: Verify Insertion**\n",
    "- Fetches returned row with `db_cursor.fetchone()`\n",
    "- Asserts returned values match expected:\n",
    "  - Row exists (not None)\n",
    "  - run_id matches generated UUID\n",
    "  - phase is 'START'\n",
    "  - status is 'OK'\n",
    "- ‚úÖ **Success:** All assertions pass\n",
    "- ‚ùå **Failure:** Any assertion fails or INSERT error\n",
    "\n",
    "**Step 5: Cleanup Test Data**\n",
    "```sql\n",
    "DELETE FROM bronze.load_log WHERE run_id = %s\n",
    "```\n",
    "- Removes test record to keep log clean\n",
    "- Uses run_id for precise deletion (no side effects)\n",
    "- Prevents test pollution in production-like environments\n",
    "- Good practice: tests should be idempotent and self-cleaning\n",
    "\n",
    "**üß™ Integration Test Value:**\n",
    "\n",
    "**What This Test Validates:**\n",
    "- **Table writability:** Can INSERT rows successfully\n",
    "- **Sequence functionality:** id auto-increments correctly\n",
    "- **Default values:** started_at populated automatically\n",
    "- **Constraint enforcement:** CHECK constraints allow valid values\n",
    "- **UUID handling:** psycopg2 adapter works correctly\n",
    "- **Query functionality:** Can retrieve inserted data\n",
    "- **Cleanup capability:** Can delete test data\n",
    "\n",
    "**What This Test Catches:**\n",
    "- **Permission issues:** User lacks INSERT privilege\n",
    "- **Type mismatches:** UUID adapter not registered\n",
    "- **Constraint violations:** Invalid phase/status values\n",
    "- **Default failures:** clock_timestamp() not working\n",
    "- **Sequence errors:** BIGSERIAL sequence broken\n",
    "\n",
    "**Why Minimal Column Set:**\n",
    "- Tests essential functionality only\n",
    "- Doesn't require nullable columns (table_name, file_path, etc.)\n",
    "- Faster execution (fewer columns to populate)\n",
    "- Focuses on core requirements (run_id, phase, status)\n",
    "- Leaves optional fields for real ETL operations\n",
    "\n",
    "**Production Usage Pattern:**\n",
    "```sql\n",
    "-- Real ETL batch would create multiple entries:\n",
    "INSERT INTO bronze.load_log (run_id, phase, status, message)\n",
    "VALUES (@run_id, 'START', 'OK', 'Starting bronze layer refresh');\n",
    "\n",
    "INSERT INTO bronze.load_log (run_id, phase, table_name, status)\n",
    "VALUES (@run_id, 'TRUNCATE', 'bronze.crm_cust_info', 'OK');\n",
    "\n",
    "INSERT INTO bronze.load_log (run_id, phase, table_name, file_path, rows_loaded, status)\n",
    "VALUES (@run_id, 'COPY', 'bronze.crm_cust_info', '/data/cust_info.csv', 15234, 'OK');\n",
    "\n",
    "INSERT INTO bronze.load_log (run_id, phase, status, message)\n",
    "VALUES (@run_id, 'FINISH', 'OK', 'Batch completed successfully');\n",
    "```\n",
    "\n",
    "**Self-Cleaning Tests:**\n",
    "- Production audit tables should never be cleaned\n",
    "- Test environments need cleanup to avoid clutter\n",
    "- run_id-based deletion is safe (no accidental deletions)\n",
    "- Alternative: Use separate test schema or database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85525e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_insert_and_query collected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_insert_and_query \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_log_insert_and_query(db_cursor):\n",
    "    \"\"\"Test inserting a record and querying it back.\"\"\"\n",
    "    import uuid\n",
    "    from psycopg2.extras import register_uuid\n",
    "    \n",
    "    # Register UUID type adapter for psycopg2\n",
    "    register_uuid()\n",
    "    \n",
    "    # Generate a unique run_id for this test\n",
    "    test_run_id = uuid.uuid4()\n",
    "    \n",
    "    # Insert a test record\n",
    "    db_cursor.execute(\"\"\"\n",
    "        INSERT INTO bronze.load_log (\n",
    "            run_id, phase, status, message\n",
    "        ) VALUES (\n",
    "            %s, 'START', 'OK', 'Test run from pytest'\n",
    "        )\n",
    "        RETURNING id, run_id, phase, status\n",
    "    \"\"\", (test_run_id,))\n",
    "    \n",
    "    result = db_cursor.fetchone()\n",
    "    assert result is not None, \"Insert should return a row\"\n",
    "    assert result[1] == test_run_id, \"run_id should match\"\n",
    "    assert result[2] == 'START', \"phase should be START\"\n",
    "    assert result[3] == 'OK', \"status should be OK\"\n",
    "    \n",
    "    # Clean up test data\n",
    "    db_cursor.execute(\"\"\"\n",
    "        DELETE FROM bronze.load_log \n",
    "        WHERE run_id = %s\n",
    "    \"\"\", (test_run_id,))\n",
    "    \n",
    "    print(f\"‚úÖ Successfully inserted and deleted test record with run_id: {test_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062edf54",
   "metadata": {},
   "source": [
    "## Summary: Run All Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62c6a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_bronze\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_insert_and_query collected 1 item\n",
      "\n",
      "t_a9c6325c3b224f11b58997d527c0ace9.py::test_load_log_insert_and_query \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run all tests in this notebook\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddf3a2",
   "metadata": {},
   "source": [
    "## Manual Inspection Queries\n",
    "\n",
    "Use these queries to manually inspect the table structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7119495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and display table structure\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "# View all columns\n",
    "df_columns = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable,\n",
    "        column_default\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'bronze'\n",
    "      AND table_name = 'load_log'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüìä bronze.load_log Columns:\")\n",
    "display(df_columns)\n",
    "\n",
    "# View all indexes\n",
    "df_indexes = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        indexname,\n",
    "        indexdef\n",
    "    FROM pg_indexes\n",
    "    WHERE schemaname = 'bronze'\n",
    "      AND tablename = 'load_log'\n",
    "    ORDER BY indexname\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüìá bronze.load_log Indexes:\")\n",
    "display(df_indexes)\n",
    "\n",
    "# View constraints\n",
    "df_constraints = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        conname AS constraint_name,\n",
    "        contype AS constraint_type,\n",
    "        pg_get_constraintdef(oid) AS definition\n",
    "    FROM pg_constraint\n",
    "    WHERE conrelid = 'bronze.load_log'::regclass\n",
    "    ORDER BY conname\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüîí bronze.load_log Constraints:\")\n",
    "display(df_constraints)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
