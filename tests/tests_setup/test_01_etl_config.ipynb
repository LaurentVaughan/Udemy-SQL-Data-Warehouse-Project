{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e09e258",
   "metadata": {},
   "source": [
    "# Test Suite: public.etl_config Configuration Table\n",
    "\n",
    "**Purpose:** Validate the structure, seeding, and behavior of `public.etl_config` table\n",
    "\n",
    "**Scope:**\n",
    "- Table existence and schema placement\n",
    "- Column definitions (names, types, constraints)\n",
    "- Required configuration keys are seeded\n",
    "- Path convention compliance (no trailing slashes)\n",
    "- Idempotency of re-runs\n",
    "- Primary key constraint enforcement\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Structural validation (table, columns, constraints)\n",
    "- Data validation (required keys, value formats)\n",
    "- Behavioral testing (idempotency, conflict handling)\n",
    "- Integration testing (path resolution)\n",
    "\n",
    "**Prerequisites:**\n",
    "- âœ… Database `sql_retail_analytics_warehouse` exists (run `setup/create_db.sql`)\n",
    "- âœ… Database connection configured (credentials in `.env` file)\n",
    "- âš ï¸ **REQUIRED:** `setup/seed/01_etl_config.sql` must be executed BEFORE running these tests\n",
    "- âœ… Required packages installed: psycopg2, pytest, ipytest, pandas\n",
    "\n",
    "**âš ï¸ If tests fail with \"table must exist\":**\n",
    "Run this first in your PostgreSQL client:\n",
    "```sql\n",
    "\\c sql_retail_analytics_warehouse\n",
    "\\i setup/seed/01_etl_config.sql\n",
    "```\n",
    "\n",
    "Or use the diagnostic cell below to check table existence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873d3a6",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies & Configure Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a427d1",
   "metadata": {},
   "source": [
    "### ðŸ” Database Password Configuration\n",
    "\n",
    "**Option 1: Environment Variable (Recommended)**\n",
    "```powershell\n",
    "# Set for current session\n",
    "$env:POSTGRES_PASSWORD = \"your_actual_password\"\n",
    "\n",
    "# Set permanently (requires new terminal)\n",
    "[System.Environment]::SetEnvironmentVariable('POSTGRES_PASSWORD', 'your_actual_password', 'User')\n",
    "```\n",
    "\n",
    "**Option 2: Update Default in Code**\n",
    "- Edit the cell below and change `'postgres'` to your actual password\n",
    "- âš ï¸ **Do NOT commit your password to git!**\n",
    "\n",
    "**Option 3: Create .env file (Best for team projects)**\n",
    "- Create `.env` file in project root (already in .gitignore)\n",
    "- Add: `POSTGRES_PASSWORD=your_actual_password`\n",
    "- Install python-dotenv: `pip install python-dotenv`\n",
    "- Load in notebook: `from dotenv import load_dotenv; load_dotenv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e75cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies imported successfully\n",
      "ðŸ”Œ Connecting to: localhost/sql_retail_analytics_warehouse as postgres\n",
      "ðŸ”‘ Password loaded from: .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pytest\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure ipytest for notebook usage\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "    'database': os.getenv('POSTGRES_DB', 'sql_retail_analytics_warehouse'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'postgres')\n",
    "}\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully\")\n",
    "print(f\"ðŸ”Œ Connecting to: {DB_CONFIG['host']}/{DB_CONFIG['database']} as {DB_CONFIG['user']}\")\n",
    "print(f\"ðŸ”‘ Password loaded from: {'.env file' if os.getenv('POSTGRES_PASSWORD') else 'default value (update .env file!)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68258a07",
   "metadata": {},
   "source": [
    "## Fixtures: Database Connection & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8531820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Database connection successful\n",
      "âœ… public.etl_config table exists\n",
      "âœ… Table has 2 row(s) of data:\n",
      "   - base_path_crm: C:/Users/Laurent/Studies/sql-ultimate-course/Udemy-SQL-Data-Warehouse-Project/datasets/source_crm\n",
      "   - base_path_erp: C:/Users/Laurent/Studies/sql-ultimate-course/Udemy-SQL-Data-Warehouse-Project/datasets/source_erp\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Try to connect\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    print(\"âœ… Database connection successful\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if etl_config table exists\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' \n",
    "          AND table_name = 'etl_config'\n",
    "    \"\"\")\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "    \n",
    "    if table_exists == 1:\n",
    "        print(\"âœ… public.etl_config table exists\")\n",
    "        \n",
    "        # Show current data\n",
    "        cursor.execute(\"SELECT * FROM public.etl_config ORDER BY config_key\")\n",
    "        rows = cursor.fetchall()\n",
    "        if rows:\n",
    "            print(f\"âœ… Table has {len(rows)} row(s) of data:\")\n",
    "            for row in rows:\n",
    "                print(f\"   - {row[0]}: {row[1]}\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Table exists but is EMPTY - run setup/seed/01_etl_config.sql\")\n",
    "    else:\n",
    "        print(\"âŒ public.etl_config table DOES NOT EXIST\")\n",
    "        print(\"\\nðŸ”§ To fix this, run the following SQL script:\")\n",
    "        print(\"   1. Open your PostgreSQL client (psql, pgAdmin, DBeaver, etc.)\")\n",
    "        print(\"   2. Connect to database: sql_retail_analytics_warehouse\")\n",
    "        print(\"   3. Execute: setup/seed/01_etl_config.sql\")\n",
    "        print(\"\\n   Example using psql:\")\n",
    "        print(\"   psql -U postgres -d sql_retail_analytics_warehouse -f setup/seed/01_etl_config.sql\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\"âŒ Database connection FAILED: {e}\")\n",
    "    print(\"\\nðŸ”§ To fix this:\")\n",
    "    print(\"   1. Verify PostgreSQL server is running\")\n",
    "    print(\"   2. Check credentials in .env file\")\n",
    "    print(\"   3. Verify database exists (run setup/create_db.sql if needed)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2c246",
   "metadata": {},
   "source": [
    "## ðŸ” Diagnostic: Check Prerequisites\n",
    "\n",
    "**Run this cell first to verify prerequisites are met**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d9bf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixtures defined\n"
     ]
    }
   ],
   "source": [
    "@pytest.fixture(scope='module')\n",
    "def db_connection():\n",
    "    \"\"\"Create a database connection for tests.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def db_cursor(db_connection):\n",
    "    \"\"\"Create a cursor for executing queries.\"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "print(\"âœ… Fixtures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413297a7",
   "metadata": {},
   "source": [
    "## Test Suite 1: Table Structure Validation\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_etl_config_table_exists` - Verifies the etl_config table exists in public schema\n",
    "2. `test_etl_config_in_public_schema` - Confirms table is in public schema (not setup, bronze, etc.)\n",
    "3. `test_etl_config_column_count` - Validates exactly 2 columns exist\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries information_schema.tables:\n",
    "  - âœ… Success: count = 1 (table exists)\n",
    "  - âŒ Failure: count = 0 (table missing - **run setup/seed/01_etl_config.sql first**)\n",
    "  - **Purpose**: Fundamental check that DDL script was executed\n",
    "  \n",
    "- **Test 2** queries table schema location:\n",
    "  - âœ… Success: table_schema = 'public'\n",
    "  - âŒ Failure: table_schema â‰  'public' or NULL (table in wrong schema or doesn't exist)\n",
    "  - **Why public schema**: Configuration tables accessible across all layers (bronze, silver, gold)\n",
    "  \n",
    "- **Test 3** counts columns:\n",
    "  - âœ… Success: count = 2 (config_key, config_value)\n",
    "  - âŒ Failure: count â‰  2 (schema mismatch or table structure changed)\n",
    "  - **Purpose**: Detects schema drift or incomplete DDL execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa34216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_table_exists collected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_table_exists \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_in_public_schema \u001b[32mPASSED\u001b[0m\u001b[32m               [ 66%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_column_count \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_in_public_schema \u001b[32mPASSED\u001b[0m\u001b[32m               [ 66%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_column_count \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_etl_config_table_exists(db_cursor):\n",
    "    \"\"\"Verify public.etl_config table exists.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' \n",
    "          AND table_name = 'etl_config'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"public.etl_config table must exist\"\n",
    "\n",
    "def test_etl_config_in_public_schema(db_cursor):\n",
    "    \"\"\"Verify table is in public schema, not other schemas.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_schema\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_name = 'etl_config'\n",
    "    \"\"\")\n",
    "    result = db_cursor.fetchone()\n",
    "    assert result is not None, \"etl_config table not found\"\n",
    "    assert result[0] == 'public', \"etl_config must be in public schema\"\n",
    "\n",
    "def test_etl_config_column_count(db_cursor):\n",
    "    \"\"\"Verify table has exactly 2 columns.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_schema = 'public' \n",
    "          AND table_name = 'etl_config'\n",
    "    \"\"\")\n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 2, \"etl_config should have exactly 2 columns (config_key, config_value)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb56a7",
   "metadata": {},
   "source": [
    "## Test Suite 2: Column Definitions\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_etl_config_column_definitions` - Validates column names, data types, lengths, and nullability\n",
    "2. `test_etl_config_primary_key` - Verifies config_key is the primary key\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries column metadata from information_schema.columns:\n",
    "  - **config_key validation**:\n",
    "    - âœ… Type: character varying (VARCHAR)\n",
    "    - âœ… Max length: 100\n",
    "    - âœ… Nullable: NO (primary key constraint)\n",
    "    - âŒ Failure: Any mismatch in type, length, or nullability\n",
    "  \n",
    "  - **config_value validation**:\n",
    "    - âœ… Type: character varying (VARCHAR)\n",
    "    - âœ… Max length: 200\n",
    "    - âœ… Nullable: NO (NOT NULL constraint)\n",
    "    - âŒ Failure: Any mismatch in type, length, or nullability\n",
    "  \n",
    "  - **Purpose**: Ensures DDL matches specification (prevents silent schema changes)\n",
    "  \n",
    "- **Test 2** queries primary key from pg_index catalog:\n",
    "  - âœ… Success: pk_columns = ['config_key']\n",
    "  - âŒ Failure: No PK, wrong column, or composite PK\n",
    "  - **Purpose**: Validates uniqueness constraint on config_key (prevents duplicate configuration entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33ac157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_column_definitions collected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_column_definitions \u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_primary_key \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_etl_config_primary_key \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.19s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.19s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_etl_config_column_definitions(db_cursor):\n",
    "    \"\"\"Verify column names, types, and nullability.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            character_maximum_length,\n",
    "            is_nullable\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "          AND table_name = 'etl_config'\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    \n",
    "    columns = db_cursor.fetchall()\n",
    "    column_dict = {\n",
    "        col[0]: {\n",
    "            'type': col[1], \n",
    "            'max_length': col[2], \n",
    "            'nullable': col[3]\n",
    "        } for col in columns\n",
    "    }\n",
    "    \n",
    "    # Expected: config_key VARCHAR(100) PRIMARY KEY (NOT NULL)\n",
    "    assert 'config_key' in column_dict, \"config_key column must exist\"\n",
    "    assert column_dict['config_key']['type'] == 'character varying', \\\n",
    "        \"config_key should be VARCHAR\"\n",
    "    assert column_dict['config_key']['max_length'] == 100, \\\n",
    "        \"config_key should have max length 100\"\n",
    "    assert column_dict['config_key']['nullable'] == 'NO', \\\n",
    "        \"config_key should be NOT NULL (primary key)\"\n",
    "    \n",
    "    # Expected: config_value VARCHAR(200) NOT NULL\n",
    "    assert 'config_value' in column_dict, \"config_value column must exist\"\n",
    "    assert column_dict['config_value']['type'] == 'character varying', \\\n",
    "        \"config_value should be VARCHAR\"\n",
    "    assert column_dict['config_value']['max_length'] == 200, \\\n",
    "        \"config_value should have max length 200\"\n",
    "    assert column_dict['config_value']['nullable'] == 'NO', \\\n",
    "        \"config_value should be NOT NULL\"\n",
    "\n",
    "def test_etl_config_primary_key(db_cursor):\n",
    "    \"\"\"Verify config_key is the primary key.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT a.attname\n",
    "        FROM pg_index i\n",
    "        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n",
    "        WHERE i.indrelid = 'public.etl_config'::regclass\n",
    "          AND i.indisprimary\n",
    "    \"\"\")\n",
    "    pk_columns = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert pk_columns == ['config_key'], \"Primary key should be 'config_key' only\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec5d5e",
   "metadata": {},
   "source": [
    "## Test Suite 3: Seeded Configuration Keys\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_required_config_keys_exist` - Verifies both base_path_crm and base_path_erp keys are seeded\n",
    "2. `test_config_values_not_null_or_empty` - Validates all config values are non-NULL and non-empty\n",
    "3. `test_config_values_are_valid_paths` - Ensures config values look like valid file paths\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries for required configuration keys:\n",
    "  - âœ… Success: Both 'base_path_crm' and 'base_path_erp' exist, total count = 2\n",
    "  - âŒ Failure: Missing keys or extra unexpected keys\n",
    "  - **Purpose**: Validates seed script populated required ETL source paths\n",
    "  - **Why these keys**: CRM and ERP are the two source systems for the data warehouse\n",
    "  \n",
    "- **Test 2** checks for NULL or empty values:\n",
    "  - âœ… Success: Query returns 0 rows (all values populated)\n",
    "  - âŒ Failure: Any rows returned (indicates NULL or empty string values)\n",
    "  - **Purpose**: Prevents runtime errors from missing configuration\n",
    "  - **TRIM check**: Catches whitespace-only values that would fail in production\n",
    "  \n",
    "- **Test 3** validates path structure:\n",
    "  - Checks each config value contains path separators ('/' or '\\')\n",
    "  - Verifies value is not just a separator (e.g., not just '/' or '\\')\n",
    "  - âœ… Success: All values contain valid path components\n",
    "  - âŒ Failure: Value doesn't look like a file path\n",
    "  - **Purpose**: Early detection of malformed paths that would cause ETL failures\n",
    "  - **Cross-platform**: Accepts both forward slashes (Unix/Linux) and backslashes (Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591d0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_required_config_keys_exist collected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_required_config_keys_exist \u001b[32mPASSED\u001b[0m\u001b[32m                [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_values_not_null_or_empty \u001b[32mPASSED\u001b[0m\u001b[32m           [ 66%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_values_are_valid_paths \u001b[32mPASSED\u001b[0m\u001b[32m             [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_values_not_null_or_empty \u001b[32mPASSED\u001b[0m\u001b[32m           [ 66%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_values_are_valid_paths \u001b[32mPASSED\u001b[0m\u001b[32m             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_required_config_keys_exist(db_cursor):\n",
    "    \"\"\"Verify both required configuration keys are seeded.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "        ORDER BY config_key\n",
    "    \"\"\")\n",
    "    \n",
    "    keys = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert 'base_path_crm' in keys, \"base_path_crm must be seeded\"\n",
    "    assert 'base_path_erp' in keys, \"base_path_erp must be seeded\"\n",
    "    assert len(keys) == 2, \"Exactly 2 required keys should be present\"\n",
    "\n",
    "def test_config_values_not_null_or_empty(db_cursor):\n",
    "    \"\"\"Verify all config values are non-NULL and non-empty.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_value IS NULL \n",
    "           OR TRIM(config_value) = ''\n",
    "    \"\"\")\n",
    "    \n",
    "    invalid_entries = db_cursor.fetchall()\n",
    "    assert len(invalid_entries) == 0, \\\n",
    "        f\"Found config keys with NULL or empty values: {invalid_entries}\"\n",
    "\n",
    "def test_config_values_are_valid_paths(db_cursor):\n",
    "    \"\"\"Verify config values look like valid file paths.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    \n",
    "    for key, value in db_cursor.fetchall():\n",
    "        # Should contain path separators\n",
    "        assert ('/' in value or '\\\\' in value), \\\n",
    "            f\"{key} value '{value}' doesn't look like a file path\"\n",
    "        # Should not be just a separator\n",
    "        assert len(value.strip('/\\\\')) > 0, \\\n",
    "            f\"{key} value '{value}' is invalid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf116f15",
   "metadata": {},
   "source": [
    "## Test Suite 4: Path Convention Compliance\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_no_trailing_slashes_in_paths` - Verifies CRITICAL requirement: no trailing slashes\n",
    "2. `test_paths_use_forward_slashes` - Validates cross-platform path compatibility\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** checks for trailing slashes (CRITICAL validation):\n",
    "  - For each path, validates:\n",
    "    - âœ… Success: Does NOT end with '/' (forward slash)\n",
    "    - âœ… Success: Does NOT end with '\\' (backslash)\n",
    "    - âŒ Failure: Ends with any trailing slash\n",
    "  - **Why this is CRITICAL**:\n",
    "    - Path concatenation: `base_path + '/' + filename`\n",
    "    - With trailing slash: `path//filename` (double slash - breaks some systems)\n",
    "    - Without trailing slash: `path/filename` (correct)\n",
    "  - **Real-world impact**: Trailing slashes cause file-not-found errors in ETL pipelines\n",
    "  - **Example violation**: `/datasets/source_crm/` should be `/datasets/source_crm`\n",
    "  \n",
    "- **Test 2** validates forward slash usage:\n",
    "  - âœ… Success: Path contains forward slashes '/' (cross-platform standard)\n",
    "  - âš ï¸ Warning: Path contains backslashes '\\' (Windows-specific, may fail on Linux/Mac)\n",
    "  - âŒ Failure: Path contains no slashes at all\n",
    "  - **Purpose**: Ensures portability across Windows, Linux, and macOS\n",
    "  - **PostgreSQL standard**: Forward slashes work on all platforms\n",
    "  - **Best practice**: Use forward slashes even on Windows for database-stored paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d8814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_trailing_slashes_in_paths collected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_trailing_slashes_in_paths \u001b[32mPASSED\u001b[0m\u001b[32m              [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_paths_use_forward_slashes \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m              [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_paths_use_forward_slashes \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_no_trailing_slashes_in_paths(db_cursor):\n",
    "    \"\"\"Verify CRITICAL requirement: no trailing slashes in file paths.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    \n",
    "    for key, value in db_cursor.fetchall():\n",
    "        assert not value.endswith('/'), \\\n",
    "            f\"{key} has trailing forward slash: '{value}' (VIOLATION: must not end with /)\"\n",
    "        assert not value.endswith('\\\\'), \\\n",
    "            f\"{key} has trailing backslash: '{value}' (VIOLATION: must not end with \\\\)\"\n",
    "\n",
    "def test_paths_use_forward_slashes(db_cursor):\n",
    "    \"\"\"Verify paths use forward slashes (cross-platform compatibility).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    \n",
    "    for key, value in db_cursor.fetchall():\n",
    "        # Should use forward slashes (PostgreSQL/cross-platform standard)\n",
    "        assert '/' in value, f\"{key} should use forward slashes: '{value}'\"\n",
    "        # Warn if backslashes are present (Windows-specific)\n",
    "        if '\\\\' in value:\n",
    "            print(f\"âš ï¸  Warning: {key} contains backslashes: '{value}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c94aa",
   "metadata": {},
   "source": [
    "## Test Suite 5: Idempotency & Conflict Handling\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_idempotent_rerun_no_duplicates` - Verifies re-running INSERT doesn't create duplicates\n",
    "2. `test_existing_values_preserved_on_conflict` - Confirms ON CONFLICT DO NOTHING preserves original values\n",
    "3. `test_primary_key_constraint_enforced` - Validates PK prevents duplicates without ON CONFLICT\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** tests idempotent INSERT behavior:\n",
    "  1. Count existing rows before re-inserting\n",
    "  2. Execute INSERT with ON CONFLICT (config_key) DO NOTHING\n",
    "  3. Count again - should match original count\n",
    "  - âœ… Success: count_before == count_after (no duplicates created)\n",
    "  - âŒ Failure: count_after > count_before (duplicates created - ON CONFLICT not working)\n",
    "  - **Purpose**: Validates seed script can be run multiple times safely\n",
    "  - **DevOps impact**: Enables repeatable deployments without manual cleanup\n",
    "  \n",
    "- **Test 2** verifies values aren't overwritten:\n",
    "  1. Capture original values\n",
    "  2. Attempt INSERT with different values using ON CONFLICT DO NOTHING\n",
    "  3. Verify values unchanged\n",
    "  - âœ… Success: original_values == current_values (values preserved)\n",
    "  - âŒ Failure: Values changed (ON CONFLICT DO UPDATE incorrectly used, or missing ON CONFLICT)\n",
    "  - **Purpose**: Prevents accidental overwriting of production configuration\n",
    "  - **Safety**: Existing environment-specific paths remain intact on re-deployment\n",
    "  \n",
    "- **Test 3** validates primary key enforcement WITHOUT ON CONFLICT:\n",
    "  - Attempts INSERT of duplicate key without ON CONFLICT clause\n",
    "  - âœ… Success: Raises psycopg2.errors.UniqueViolation exception\n",
    "  - âŒ Failure: INSERT succeeds (primary key constraint missing or not enforced)\n",
    "  - **Purpose**: Confirms database-level constraint works independently of application logic\n",
    "  - **Database integrity**: PK constraint is last line of defense against duplicates\n",
    "  - **Technical**: Uses pytest.raises() to assert exception is thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc255b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_idempotent_rerun_no_duplicates collected 3 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_idempotent_rerun_no_duplicates \u001b[32mPASSED\u001b[0m\u001b[32m            [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_existing_values_preserved_on_conflict \u001b[32mPASSED\u001b[0m\u001b[32m     [ 66%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m            [ 33%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_existing_values_preserved_on_conflict \u001b[32mPASSED\u001b[0m\u001b[32m     [ 66%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_primary_key_constraint_enforced \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_primary_key_constraint_enforced \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_idempotent_rerun_no_duplicates(db_cursor):\n",
    "    \"\"\"Verify re-running INSERT with ON CONFLICT doesn't create duplicates.\"\"\"\n",
    "    # Count before\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    count_before = db_cursor.fetchone()[0]\n",
    "    \n",
    "    # Re-run the INSERT (should do nothing due to ON CONFLICT)\n",
    "    db_cursor.execute(\"\"\"\n",
    "        INSERT INTO public.etl_config (config_key, config_value)\n",
    "        VALUES\n",
    "          ('base_path_crm', 'dummy_path_1'),\n",
    "          ('base_path_erp', 'dummy_path_2')\n",
    "        ON CONFLICT (config_key) DO NOTHING\n",
    "    \"\"\")\n",
    "    \n",
    "    # Count after\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    count_after = db_cursor.fetchone()[0]\n",
    "    \n",
    "    assert count_before == count_after, \\\n",
    "        f\"Idempotency violated: count changed from {count_before} to {count_after}\"\n",
    "\n",
    "def test_existing_values_preserved_on_conflict(db_cursor):\n",
    "    \"\"\"Verify ON CONFLICT DO NOTHING preserves existing values.\"\"\"\n",
    "    # Get original values\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "        ORDER BY config_key\n",
    "    \"\"\")\n",
    "    original_values = {row[0]: row[1] for row in db_cursor.fetchall()}\n",
    "    \n",
    "    # Attempt to insert different values (should be ignored)\n",
    "    db_cursor.execute(\"\"\"\n",
    "        INSERT INTO public.etl_config (config_key, config_value)\n",
    "        VALUES\n",
    "          ('base_path_crm', 'THIS_SHOULD_BE_IGNORED'),\n",
    "          ('base_path_erp', 'THIS_SHOULD_ALSO_BE_IGNORED')\n",
    "        ON CONFLICT (config_key) DO NOTHING\n",
    "    \"\"\")\n",
    "    \n",
    "    # Verify values unchanged\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "        ORDER BY config_key\n",
    "    \"\"\")\n",
    "    current_values = {row[0]: row[1] for row in db_cursor.fetchall()}\n",
    "    \n",
    "    assert original_values == current_values, \\\n",
    "        \"ON CONFLICT DO NOTHING failed to preserve existing values\"\n",
    "\n",
    "def test_primary_key_constraint_enforced(db_cursor):\n",
    "    \"\"\"Verify primary key prevents duplicate keys without ON CONFLICT.\"\"\"\n",
    "    from psycopg2 import errors\n",
    "    \n",
    "    with pytest.raises(errors.UniqueViolation):\n",
    "        db_cursor.execute(\"\"\"\n",
    "            INSERT INTO public.etl_config (config_key, config_value)\n",
    "            VALUES ('base_path_crm', 'duplicate_attempt')\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318408d5",
   "metadata": {},
   "source": [
    "## Test Suite 6: Integration - Path Construction\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_path_construction_no_double_slashes` - Validates path concatenation doesn't create double slashes\n",
    "2. `test_realistic_file_path_construction` - Tests path building with real CSV filenames\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** simulates runtime path concatenation in SQL:\n",
    "  - Constructs full paths using SQL concatenation: `config_value || '/' || 'test_file.csv'`\n",
    "  - For each constructed path:\n",
    "    - âœ… Success: No '//' (double slash) in full_path\n",
    "    - âœ… Success: Path ends with 'test_file.csv'\n",
    "    - âŒ Failure: Contains '//' (indicates trailing slash in base_path)\n",
    "  - **Purpose**: Simulates actual ETL path construction logic\n",
    "  - **Why test in SQL**: Validates database-side path building (used in COPY commands)\n",
    "  - **Example good**: `'/datasets/source_crm' || '/' || 'cust_info.csv'` = `/datasets/source_crm/cust_info.csv` âœ…\n",
    "  - **Example bad**: `'/datasets/source_crm/' || '/' || 'cust_info.csv'` = `/datasets/source_crm//cust_info.csv` âŒ\n",
    "  \n",
    "- **Test 2** tests with realistic CRM filenames:\n",
    "  - Test files: ['cust_info.csv', 'prd_info.csv', 'CUST_AZ12.csv']\n",
    "  - Fetches base_path_crm from database\n",
    "  - Constructs full paths using Python f-string: `f\"{base_path}/{filename}\"`\n",
    "  - Validates each constructed path:\n",
    "    - âœ… Success: No '//' in path\n",
    "    - âœ… Success: Path ends with expected filename\n",
    "    - âŒ Failure: Double slashes or filename corruption\n",
    "  - **Purpose**: End-to-end validation with actual ETL source filenames\n",
    "  - **Real-world simulation**: Uses same path construction logic as Bronze layer ETL\n",
    "  - **Coverage**: Tests both standard naming (cust_info.csv) and uppercase variants (CUST_AZ12.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9a006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_path_construction_no_double_slashes collected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_path_construction_no_double_slashes \u001b[32mPASSED\u001b[0m\u001b[32m       [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_realistic_file_path_construction \u001b[32mPASSED\u001b[0m\u001b[32m          [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m       [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_realistic_file_path_construction \u001b[32mPASSED\u001b[0m\u001b[32m          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_path_construction_no_double_slashes(db_cursor):\n",
    "    \"\"\"Verify path concatenation doesn't create double slashes.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            config_key,\n",
    "            config_value,\n",
    "            config_value || '/' || 'test_file.csv' AS full_path\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    \n",
    "    for key, base_path, full_path in db_cursor.fetchall():\n",
    "        assert '//' not in full_path, \\\n",
    "            f\"{key}: Path construction created double slash: '{full_path}'\"\n",
    "        assert full_path.endswith('test_file.csv'), \\\n",
    "            f\"{key}: Path construction failed: '{full_path}'\"\n",
    "\n",
    "def test_realistic_file_path_construction(db_cursor):\n",
    "    \"\"\"Test path construction with realistic filenames.\"\"\"\n",
    "    test_files = ['cust_info.csv', 'prd_info.csv', 'CUST_AZ12.csv']\n",
    "    \n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key = 'base_path_crm'\n",
    "    \"\"\")\n",
    "    base_path = db_cursor.fetchone()[0]\n",
    "    \n",
    "    for filename in test_files:\n",
    "        full_path = f\"{base_path}/{filename}\"\n",
    "        assert '//' not in full_path, f\"Double slash in: {full_path}\"\n",
    "        assert full_path.endswith(filename), f\"Filename lost: {full_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c2386",
   "metadata": {},
   "source": [
    "## Test Suite 7: Data Integrity\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_no_duplicate_keys` - Verifies no duplicate config_key values exist\n",
    "2. `test_config_key_case_sensitivity` - Validates keys use consistent snake_case convention\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** checks for duplicates using GROUP BY with HAVING:\n",
    "  - Groups by config_key and counts occurrences\n",
    "  - Filters to show only keys appearing more than once\n",
    "  - âœ… Success: Query returns 0 rows (no duplicates)\n",
    "  - âŒ Failure: Any rows returned (indicates duplicate keys)\n",
    "  - **Purpose**: Validates primary key constraint is working\n",
    "  - **Redundant check**: Yes, but important for data integrity auditing\n",
    "  - **Detects issues**: Catches edge cases like constraint disabled/dropped\n",
    "  - **Example violation**: Both 'base_path_crm' and 'BASE_PATH_CRM' exist (PostgreSQL identifiers are case-sensitive in data)\n",
    "  \n",
    "- **Test 2** validates naming convention compliance:\n",
    "  - Fetches all config keys\n",
    "  - For each key, validates:\n",
    "    - âœ… Success: key == key.lower() (all lowercase)\n",
    "    - âœ… Success: No spaces in key\n",
    "    - âŒ Failure: Mixed case (e.g., 'base_Path_CRM')\n",
    "    - âŒ Failure: Contains spaces (e.g., 'base path crm')\n",
    "  - **Purpose**: Enforces consistent naming convention (snake_case)\n",
    "  - **Why it matters**:\n",
    "    - Prevents confusion: 'base_path_crm' vs 'Base_Path_CRM' vs 'BASE_PATH_CRM'\n",
    "    - Easier SQL queries: No need to remember exact casing\n",
    "    - Standard convention: snake_case is Python/PostgreSQL community standard\n",
    "  - **Convention**: lowercase_with_underscores (snake_case)\n",
    "  - **Alternative rejected**: camelCase (not PostgreSQL convention), UPPER_CASE (harder to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c05dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_duplicate_keys collected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_duplicate_keys \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_key_case_sensitivity \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_key_case_sensitivity \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_no_duplicate_keys(db_cursor):\n",
    "    \"\"\"Verify no duplicate config_key values exist.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, COUNT(*)\n",
    "        FROM public.etl_config\n",
    "        GROUP BY config_key\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    duplicates = db_cursor.fetchall()\n",
    "    assert len(duplicates) == 0, f\"Found duplicate keys: {duplicates}\"\n",
    "\n",
    "def test_config_key_case_sensitivity(db_cursor):\n",
    "    \"\"\"Verify config keys use consistent casing convention.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    \"\"\")\n",
    "    \n",
    "    keys = [row[0] for row in db_cursor.fetchall()]\n",
    "    for key in keys:\n",
    "        # Verify lowercase with underscores (snake_case)\n",
    "        assert key == key.lower(), f\"Key '{key}' should be lowercase\"\n",
    "        assert ' ' not in key, f\"Key '{key}' should not contain spaces\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961e84a",
   "metadata": {},
   "source": [
    "## Summary: Run All Tests\n",
    "\n",
    "**Executes all test suites (15 tests total):**\n",
    "- Suite 1: Table Structure Validation (3 tests)\n",
    "- Suite 2: Column Definitions (2 tests)\n",
    "- Suite 3: Seeded Configuration Keys (3 tests)\n",
    "- Suite 4: Path Convention Compliance (2 tests)\n",
    "- Suite 5: Idempotency & Conflict Handling (3 tests)\n",
    "- Suite 6: Integration - Path Construction (2 tests)\n",
    "- Suite 7: Data Integrity (2 tests)\n",
    "\n",
    "**How this cell works:**\n",
    "- Executes `ipytest.run('-vv')` which runs all pytest functions defined in this notebook\n",
    "- `-vv` flag provides **very verbose** output showing:\n",
    "  - Each test function name as it runs\n",
    "  - PASSED/FAILED status for each test\n",
    "  - Detailed assertion messages on failure\n",
    "  - Full traceback on errors\n",
    "  - Percentage completion progress\n",
    "\n",
    "**âœ… Success Criteria:**\n",
    "- All 15 tests show `PASSED` status\n",
    "- Final summary shows: `15 passed in X.XXs`\n",
    "- No `FAILED`, `ERROR`, or `SKIPPED` statuses\n",
    "\n",
    "**ðŸ”§ Troubleshooting Test Failures:**\n",
    "\n",
    "| Failure Type | Likely Cause | Solution |\n",
    "|-------------|--------------|----------|\n",
    "| Table doesn't exist | Seed script not run | Execute `setup/seed/01_etl_config.sql` |\n",
    "| Column mismatch | Schema drift or wrong version | Verify DDL matches specification |\n",
    "| Missing config keys | Incomplete seed data | Check INSERT statements in seed script |\n",
    "| Trailing slash in path | Manual UPDATE or wrong seed | Fix paths: `UPDATE public.etl_config SET config_value = RTRIM(config_value, '/') WHERE config_key IN ('base_path_crm', 'base_path_erp')` |\n",
    "| Primary key violation | Constraint missing | Add constraint: `ALTER TABLE public.etl_config ADD PRIMARY KEY (config_key)` |\n",
    "| Duplicate keys found | Data corruption | Investigate and remove duplicates manually |\n",
    "\n",
    "**ðŸ“Š What Good Output Looks Like:**\n",
    "```\n",
    "============================= test session starts =============================\n",
    "collected 15 items\n",
    "\n",
    "test_01_etl_config.py::test_etl_config_table_exists PASSED              [  6%]\n",
    "test_01_etl_config.py::test_etl_config_in_public_schema PASSED          [ 13%]\n",
    "...\n",
    "test_01_etl_config.py::test_config_key_case_sensitivity PASSED          [100%]\n",
    "============================== 15 passed in 0.45s ==============================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0b9e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_duplicate_keys collected 2 items\n",
      "\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_no_duplicate_keys \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_key_case_sensitivity \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_e25fc1d24d884774a37c61e478b34a90.py::test_config_key_case_sensitivity \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run all tests in this notebook\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c17d0e",
   "metadata": {},
   "source": [
    "## Manual Inspection: View Configuration\n",
    "\n",
    "**What this cell does:**\n",
    "1. **Current Configuration** - Displays all config entries with path validation checks\n",
    "2. **Table Structure** - Shows column definitions, types, lengths, and constraints\n",
    "3. **Constraints** - Lists all table constraints (PRIMARY KEY, etc.)\n",
    "\n",
    "**How this cell works:**\n",
    "\n",
    "**Step 1: Current Configuration Display**\n",
    "- Queries all config entries with validation indicators\n",
    "- **Output format**: DataFrame with validation indicators\n",
    "- **Expected values**:\n",
    "  - config_key: 'base_path_crm', 'base_path_erp'\n",
    "  - config_value: Full paths without trailing slashes\n",
    "  - value_length: Typically 20-100 characters\n",
    "  - path_check: All should show 'âœ… No trailing slash'\n",
    "- **Use case**: Quick visual verification of configuration state\n",
    "\n",
    "**Step 2: Table Structure**\n",
    "- Queries column metadata from information_schema\n",
    "- **Output format**: DataFrame showing schema definition\n",
    "- **Expected columns**:\n",
    "  1. config_key: character varying(100), NOT NULL, no default\n",
    "  2. config_value: character varying(200), NOT NULL, no default\n",
    "- **Purpose**: Verify DDL matches documentation\n",
    "\n",
    "**Step 3: Constraints**\n",
    "- Queries constraint definitions from pg_constraint catalog\n",
    "- **Output format**: DataFrame with constraint details\n",
    "- **Expected constraints**:\n",
    "  - Primary Key on config_key\n",
    "  - Possibly named 'etl_config_pkey' or similar\n",
    "- **Purpose**: Validates database-level integrity constraints are in place\n",
    "\n",
    "**Expected console output:**\n",
    "```\n",
    "ðŸ“‹ Current Configuration:\n",
    "[DataFrame showing config_key, config_value, value_length, path_check]\n",
    "\n",
    "ðŸ—ï¸  Table Structure:\n",
    "[DataFrame showing column definitions]\n",
    "\n",
    "ðŸ”’ Constraints:\n",
    "[DataFrame showing PRIMARY KEY constraint]\n",
    "\n",
    "âœ… Inspection complete\n",
    "```\n",
    "\n",
    "**Use this for:**\n",
    "- Visual confirmation of test assertions\n",
    "- Debugging path configuration issues\n",
    "- Verifying schema matches specification\n",
    "- Documentation/screenshots for handoffs\n",
    "- Troubleshooting trailing slash violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b289fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Current Configuration:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_23092\\1195893564.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_config = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "config_key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "config_value",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "value_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "path_check",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "eed26175-a608-4c4f-bef6-edbd34bbf374",
       "rows": [
        [
         "0",
         "base_path_crm",
         "C:/Users/Laurent/Studies/sql-ultimate-course/Udemy-SQL-Data-Warehouse-Project/datasets/source_crm",
         "97",
         "âœ… No trailing slash"
        ],
        [
         "1",
         "base_path_erp",
         "C:/Users/Laurent/Studies/sql-ultimate-course/Udemy-SQL-Data-Warehouse-Project/datasets/source_erp",
         "97",
         "âœ… No trailing slash"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_key</th>\n",
       "      <th>config_value</th>\n",
       "      <th>value_length</th>\n",
       "      <th>path_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_path_crm</td>\n",
       "      <td>C:/Users/Laurent/Studies/sql-ultimate-course/U...</td>\n",
       "      <td>97</td>\n",
       "      <td>âœ… No trailing slash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base_path_erp</td>\n",
       "      <td>C:/Users/Laurent/Studies/sql-ultimate-course/U...</td>\n",
       "      <td>97</td>\n",
       "      <td>âœ… No trailing slash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      config_key                                       config_value  \\\n",
       "0  base_path_crm  C:/Users/Laurent/Studies/sql-ultimate-course/U...   \n",
       "1  base_path_erp  C:/Users/Laurent/Studies/sql-ultimate-course/U...   \n",
       "\n",
       "   value_length           path_check  \n",
       "0            97  âœ… No trailing slash  \n",
       "1            97  âœ… No trailing slash  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—ï¸  Table Structure:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_23092\\1195893564.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_structure = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "column_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "data_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "character_maximum_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_nullable",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "column_default",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "f6bac2df-f7a1-43a2-96c1-309304f1ee41",
       "rows": [
        [
         "0",
         "config_key",
         "character varying",
         "100",
         "NO",
         null
        ],
        [
         "1",
         "config_value",
         "character varying",
         "200",
         "NO",
         null
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>character_maximum_length</th>\n",
       "      <th>is_nullable</th>\n",
       "      <th>column_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>config_key</td>\n",
       "      <td>character varying</td>\n",
       "      <td>100</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>config_value</td>\n",
       "      <td>character varying</td>\n",
       "      <td>200</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_name          data_type  character_maximum_length is_nullable  \\\n",
       "0    config_key  character varying                       100          NO   \n",
       "1  config_value  character varying                       200          NO   \n",
       "\n",
       "  column_default  \n",
       "0           None  \n",
       "1           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”’ Constraints:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_23092\\1195893564.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_constraints = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "constraint_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "constraint_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "definition",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "30204a26-9ff3-4b51-9ef6-8b020a4320c1",
       "rows": [
        [
         "0",
         "etl_config_config_key_not_null",
         "n",
         "NOT NULL config_key"
        ],
        [
         "1",
         "etl_config_config_value_not_null",
         "n",
         "NOT NULL config_value"
        ],
        [
         "2",
         "etl_config_pkey",
         "PRIMARY KEY",
         "PRIMARY KEY (config_key)"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>constraint_name</th>\n",
       "      <th>constraint_type</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etl_config_config_key_not_null</td>\n",
       "      <td>n</td>\n",
       "      <td>NOT NULL config_key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>etl_config_config_value_not_null</td>\n",
       "      <td>n</td>\n",
       "      <td>NOT NULL config_value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>etl_config_pkey</td>\n",
       "      <td>PRIMARY KEY</td>\n",
       "      <td>PRIMARY KEY (config_key)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    constraint_name constraint_type                definition\n",
       "0    etl_config_config_key_not_null               n       NOT NULL config_key\n",
       "1  etl_config_config_value_not_null               n     NOT NULL config_value\n",
       "2                   etl_config_pkey     PRIMARY KEY  PRIMARY KEY (config_key)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Inspection complete\n"
     ]
    }
   ],
   "source": [
    "# Connect and display current configuration\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "# View all configuration entries\n",
    "df_config = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        config_key,\n",
    "        config_value,\n",
    "        LENGTH(config_value) AS value_length,\n",
    "        CASE \n",
    "            WHEN config_value LIKE '%/' THEN 'âŒ Has trailing slash'\n",
    "            ELSE 'âœ… No trailing slash'\n",
    "        END AS path_check\n",
    "    FROM public.etl_config\n",
    "    ORDER BY config_key\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nðŸ“‹ Current Configuration:\")\n",
    "display(df_config)\n",
    "\n",
    "# View table structure\n",
    "df_structure = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        character_maximum_length,\n",
    "        is_nullable,\n",
    "        column_default\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "      AND table_name = 'etl_config'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nðŸ—ï¸  Table Structure:\")\n",
    "display(df_structure)\n",
    "\n",
    "# View constraints\n",
    "df_constraints = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        conname AS constraint_name,\n",
    "        CASE contype\n",
    "            WHEN 'p' THEN 'PRIMARY KEY'\n",
    "            WHEN 'f' THEN 'FOREIGN KEY'\n",
    "            WHEN 'u' THEN 'UNIQUE'\n",
    "            WHEN 'c' THEN 'CHECK'\n",
    "            ELSE contype::text\n",
    "        END AS constraint_type,\n",
    "        pg_get_constraintdef(oid) AS definition\n",
    "    FROM pg_constraint\n",
    "    WHERE conrelid = 'public.etl_config'::regclass\n",
    "    ORDER BY conname\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nðŸ”’ Constraints:\")\n",
    "display(df_constraints)\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nâœ… Inspection complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
