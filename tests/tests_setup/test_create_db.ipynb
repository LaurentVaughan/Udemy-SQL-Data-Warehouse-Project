{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12320785",
   "metadata": {},
   "source": [
    "# Test Suite: Database Creation and Configuration\n",
    "\n",
    "**Purpose:** Validate the sql_retail_analytics_warehouse database creation and configuration\n",
    "\n",
    "**Scope:**\n",
    "- Database existence and naming\n",
    "- Encoding configuration (UTF-8)\n",
    "- Locale settings (en_GB.UTF-8)\n",
    "- Template configuration\n",
    "- Ownership and privileges\n",
    "- Connection validation\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Existence validation (database created successfully)\n",
    "- Configuration validation (encoding, collation, ctype)\n",
    "- Ownership validation (correct owner assigned)\n",
    "- Connection testing (can establish connections)\n",
    "- Isolation testing (clean template, no extra objects)\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL server running\n",
    "- `setup/create_db.sql` has been executed\n",
    "- Connection credentials available\n",
    "- Required packages: psycopg2, pytest, ipytest, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044c638",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies & Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb006979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import pytest\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "\n",
    "# Configure ipytest for notebook usage\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'postgres',\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'your_password_here')\n",
    "}\n",
    "\n",
    "# Target database name\n",
    "TARGET_DB = 'sql_retail_analytics_warehouse'\n",
    "\n",
    "print(\"‚úÖ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e33715",
   "metadata": {},
   "source": [
    "## Fixtures: Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope='module')\n",
    "def postgres_connection():\n",
    "    \"\"\"Connection to postgres database for catalog queries.\"\"\"\n",
    "    conn = psycopg2.connect(database='postgres', **DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def postgres_cursor(postgres_connection):\n",
    "    \"\"\"Cursor for postgres database.\"\"\"\n",
    "    cursor = postgres_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def target_connection():\n",
    "    \"\"\"Connection to target warehouse database.\"\"\"\n",
    "    conn = psycopg2.connect(database=TARGET_DB, **DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def target_cursor(target_connection):\n",
    "    \"\"\"Cursor for target database.\"\"\"\n",
    "    cursor = target_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "print(\"‚úÖ Fixtures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3610f2e",
   "metadata": {},
   "source": [
    "## Test Suite 1: Database Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_exists(postgres_cursor):\n",
    "    \"\"\"Verify sql_retail_analytics_warehouse database exists.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    count = postgres_cursor.fetchone()[0]\n",
    "    assert count == 1, f\"Database '{TARGET_DB}' must exist\"\n",
    "\n",
    "def test_database_name_exact_match(postgres_cursor):\n",
    "    \"\"\"Verify database name matches exactly (case-sensitive).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datname\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    result = postgres_cursor.fetchone()\n",
    "    assert result is not None, f\"Database '{TARGET_DB}' not found\"\n",
    "    assert result[0] == TARGET_DB, f\"Database name mismatch: expected '{TARGET_DB}', got '{result[0]}'\"\n",
    "\n",
    "def test_database_is_accessible(target_cursor):\n",
    "    \"\"\"Verify we can connect to and query the database.\"\"\"\n",
    "    target_cursor.execute(\"SELECT current_database()\")\n",
    "    current_db = target_cursor.fetchone()[0]\n",
    "    assert current_db == TARGET_DB, f\"Connected to wrong database: {current_db}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add5fdc",
   "metadata": {},
   "source": [
    "## Test Suite 2: Encoding Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5433ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_encoding_utf8(postgres_cursor):\n",
    "    \"\"\"Verify database uses UTF8 encoding.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT pg_encoding_to_char(encoding) AS encoding\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    encoding = postgres_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Expected UTF8 encoding, got '{encoding}'\"\n",
    "\n",
    "def test_database_encoding_from_connection(target_cursor):\n",
    "    \"\"\"Verify encoding setting from within the database.\"\"\"\n",
    "    target_cursor.execute(\"SHOW server_encoding\")\n",
    "    encoding = target_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Server encoding should be UTF8, got '{encoding}'\"\n",
    "\n",
    "def test_client_encoding_utf8(target_cursor):\n",
    "    \"\"\"Verify client encoding is also UTF8.\"\"\"\n",
    "    target_cursor.execute(\"SHOW client_encoding\")\n",
    "    encoding = target_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Client encoding should be UTF8, got '{encoding}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97599af8",
   "metadata": {},
   "source": [
    "## Test Suite 3: Locale Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_collation_en_gb(postgres_cursor):\n",
    "    \"\"\"Verify database uses en_GB.UTF-8 collation.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datcollate\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    collation = postgres_cursor.fetchone()[0]\n",
    "    assert collation == 'en_GB.UTF-8', f\"Expected 'en_GB.UTF-8' collation, got '{collation}'\"\n",
    "\n",
    "def test_database_ctype_en_gb(postgres_cursor):\n",
    "    \"\"\"Verify database uses en_GB.UTF-8 character classification.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datctype\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    ctype = postgres_cursor.fetchone()[0]\n",
    "    assert ctype == 'en_GB.UTF-8', f\"Expected 'en_GB.UTF-8' ctype, got '{ctype}'\"\n",
    "\n",
    "def test_lc_collate_setting(target_cursor):\n",
    "    \"\"\"Verify LC_COLLATE setting from within database.\"\"\"\n",
    "    target_cursor.execute(\"SHOW lc_collate\")\n",
    "    lc_collate = target_cursor.fetchone()[0]\n",
    "    assert lc_collate == 'en_GB.UTF-8', f\"LC_COLLATE should be 'en_GB.UTF-8', got '{lc_collate}'\"\n",
    "\n",
    "def test_lc_ctype_setting(target_cursor):\n",
    "    \"\"\"Verify LC_CTYPE setting from within database.\"\"\"\n",
    "    target_cursor.execute(\"SHOW lc_ctype\")\n",
    "    lc_ctype = target_cursor.fetchone()[0]\n",
    "    assert lc_ctype == 'en_GB.UTF-8', f\"LC_CTYPE should be 'en_GB.UTF-8', got '{lc_ctype}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06783f",
   "metadata": {},
   "source": [
    "## Test Suite 4: Template Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f626a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_allows_connections(postgres_cursor):\n",
    "    \"\"\"Verify database allows connections (not a template).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datallowconn\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    allows_conn = postgres_cursor.fetchone()[0]\n",
    "    assert allows_conn is True, \"Database should allow connections\"\n",
    "\n",
    "def test_database_not_a_template(postgres_cursor):\n",
    "    \"\"\"Verify database is not marked as a template.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datistemplate\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    is_template = postgres_cursor.fetchone()[0]\n",
    "    assert is_template is False, \"Database should not be a template\"\n",
    "\n",
    "def test_no_active_connections_limit(postgres_cursor):\n",
    "    \"\"\"Verify database has no connection limit (-1 = unlimited).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datconnlimit\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    conn_limit = postgres_cursor.fetchone()[0]\n",
    "    assert conn_limit == -1, f\"Expected unlimited connections (-1), got {conn_limit}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de06c8e",
   "metadata": {},
   "source": [
    "## Test Suite 5: Ownership and Privileges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42995fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_owner(postgres_cursor):\n",
    "    \"\"\"Verify database is owned by postgres user.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT pg_catalog.pg_get_userbyid(d.datdba) AS owner\n",
    "        FROM pg_catalog.pg_database d\n",
    "        WHERE d.datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    owner = postgres_cursor.fetchone()[0]\n",
    "    # Owner should be postgres or the creating user\n",
    "    assert owner is not None, \"Database must have an owner\"\n",
    "    assert len(owner) > 0, \"Owner name should not be empty\"\n",
    "\n",
    "def test_current_user_can_create_schema(target_cursor):\n",
    "    \"\"\"Verify current user has privileges to create schemas.\"\"\"\n",
    "    # Try to create a test schema (will rollback)\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS test_privilege_check\n",
    "    \"\"\")\n",
    "    \n",
    "    # Verify it was created\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.schemata\n",
    "        WHERE schema_name = 'test_privilege_check'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = target_cursor.fetchone()[0]\n",
    "    assert count == 1, \"User should be able to create schemas\"\n",
    "    \n",
    "    # Clean up\n",
    "    target_cursor.execute(\"DROP SCHEMA IF EXISTS test_privilege_check CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b8675",
   "metadata": {},
   "source": [
    "## Test Suite 6: Clean State Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a46118",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_expected_default_schemas_only(target_cursor):\n",
    "    \"\"\"Verify only default PostgreSQL schemas exist (if using template0).\"\"\"\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT schema_name\n",
    "        FROM information_schema.schemata\n",
    "        WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "        ORDER BY schema_name\n",
    "    \"\"\")\n",
    "    \n",
    "    schemas = [row[0] for row in target_cursor.fetchall()]\n",
    "    \n",
    "    # Should only have 'public' by default (if clean template0)\n",
    "    # May have bronze/silver/gold if create_schemas.sql was run\n",
    "    # This test verifies no unexpected schemas exist\n",
    "    expected_schemas = {'public', 'bronze', 'silver', 'gold', 'setup'}\n",
    "    unexpected = set(schemas) - expected_schemas\n",
    "    \n",
    "    if unexpected:\n",
    "        print(f\"‚ö†Ô∏è  Unexpected schemas found: {unexpected}\")\n",
    "        print(f\"   All schemas: {schemas}\")\n",
    "    \n",
    "    # At minimum, 'public' should exist\n",
    "    assert 'public' in schemas, \"Default 'public' schema should exist\"\n",
    "\n",
    "def test_database_size_reasonable(target_cursor):\n",
    "    \"\"\"Verify database size is reasonable for a clean database.\"\"\"\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT pg_size_pretty(pg_database_size(current_database())) AS size,\n",
    "               pg_database_size(current_database()) AS size_bytes\n",
    "    \"\"\")\n",
    "    \n",
    "    size_pretty, size_bytes = target_cursor.fetchone()\n",
    "    \n",
    "    # Clean database should be under 20MB\n",
    "    max_size_bytes = 20 * 1024 * 1024  # 20MB\n",
    "    \n",
    "    print(f\"Database size: {size_pretty}\")\n",
    "    assert size_bytes < max_size_bytes, \\\n",
    "        f\"Database seems too large for a clean database: {size_pretty}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8ec37",
   "metadata": {},
   "source": [
    "## Test Suite 7: Connection and Session Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_timezone_setting(target_cursor):\n",
    "    \"\"\"Verify timezone is set (should have default or configured value).\"\"\"\n",
    "    target_cursor.execute(\"SHOW timezone\")\n",
    "    timezone = target_cursor.fetchone()[0]\n",
    "    assert timezone is not None, \"Timezone should be set\"\n",
    "    assert len(timezone) > 0, \"Timezone value should not be empty\"\n",
    "\n",
    "def test_datestyle_setting(target_cursor):\n",
    "    \"\"\"Verify DateStyle is set to ISO standard.\"\"\"\n",
    "    target_cursor.execute(\"SHOW datestyle\")\n",
    "    datestyle = target_cursor.fetchone()[0]\n",
    "    # Should contain 'ISO' for consistent date formatting\n",
    "    assert 'ISO' in datestyle, f\"DateStyle should include ISO, got '{datestyle}'\"\n",
    "\n",
    "def test_can_create_table(target_cursor):\n",
    "    \"\"\"Verify basic DDL operations work.\"\"\"\n",
    "    # Create a test table\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS test_table_creation (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name TEXT NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Verify it exists\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name = 'test_table_creation'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = target_cursor.fetchone()[0]\n",
    "    assert count == 1, \"Should be able to create tables\"\n",
    "    \n",
    "    # Clean up\n",
    "    target_cursor.execute(\"DROP TABLE IF EXISTS test_table_creation\")\n",
    "\n",
    "def test_can_insert_and_query_utf8_data(target_cursor):\n",
    "    \"\"\"Verify UTF-8 data can be inserted and queried correctly.\"\"\"\n",
    "    # Create temp table\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE TEMP TABLE test_utf8 (data TEXT)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert various UTF-8 characters\n",
    "    test_strings = [\n",
    "        'Hello World',\n",
    "        'Caf√©',\n",
    "        'Êó•Êú¨Ë™û',  # Japanese\n",
    "        'üéØüìä',    # Emojis\n",
    "        'ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨'  # Greek\n",
    "    ]\n",
    "    \n",
    "    for test_str in test_strings:\n",
    "        target_cursor.execute(\n",
    "            \"INSERT INTO test_utf8 (data) VALUES (%s)\",\n",
    "            (test_str,)\n",
    "        )\n",
    "    \n",
    "    # Query back and verify\n",
    "    target_cursor.execute(\"SELECT data FROM test_utf8 ORDER BY data\")\n",
    "    results = [row[0] for row in target_cursor.fetchall()]\n",
    "    \n",
    "    # All strings should be retrievable\n",
    "    assert len(results) == len(test_strings), \"All UTF-8 strings should be retrievable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419b624",
   "metadata": {},
   "source": [
    "## Summary: Run All Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests in this notebook\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6d313",
   "metadata": {},
   "source": [
    "## Manual Inspection: Database Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to postgres database to query catalog\n",
    "conn_postgres = psycopg2.connect(database='postgres', **DB_CONFIG)\n",
    "\n",
    "# Get comprehensive database information\n",
    "df_db_info = pd.read_sql(f\"\"\"\n",
    "    SELECT\n",
    "        d.datname                                  AS database_name,\n",
    "        pg_catalog.pg_get_userbyid(d.datdba)       AS owner,\n",
    "        pg_catalog.pg_encoding_to_char(d.encoding) AS encoding,\n",
    "        d.datcollate                               AS collation,\n",
    "        d.datctype                                 AS ctype,\n",
    "        d.datallowconn                             AS allows_connections,\n",
    "        d.datconnlimit                             AS connection_limit,\n",
    "        d.datistemplate                            AS is_template,\n",
    "        pg_size_pretty(pg_database_size(d.datname)) AS size\n",
    "    FROM pg_catalog.pg_database d\n",
    "    WHERE d.datname = '{TARGET_DB}'\n",
    "\"\"\", conn_postgres)\n",
    "\n",
    "print(\"\\nüóÑÔ∏è  Database Configuration:\")\n",
    "display(df_db_info.T)  # Transpose for better readability\n",
    "\n",
    "conn_postgres.close()\n",
    "\n",
    "# Connect to target database for additional info\n",
    "conn_target = psycopg2.connect(database=TARGET_DB, **DB_CONFIG)\n",
    "\n",
    "# Get session settings\n",
    "df_settings = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        'server_encoding' AS setting,\n",
    "        current_setting('server_encoding') AS value\n",
    "    UNION ALL\n",
    "    SELECT 'client_encoding', current_setting('client_encoding')\n",
    "    UNION ALL\n",
    "    SELECT 'lc_collate', current_setting('lc_collate')\n",
    "    UNION ALL\n",
    "    SELECT 'lc_ctype', current_setting('lc_ctype')\n",
    "    UNION ALL\n",
    "    SELECT 'timezone', current_setting('timezone')\n",
    "    UNION ALL\n",
    "    SELECT 'datestyle', current_setting('datestyle')\n",
    "    ORDER BY setting\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Session Settings:\")\n",
    "display(df_settings)\n",
    "\n",
    "# Get list of schemas\n",
    "df_schemas = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        schema_name,\n",
    "        pg_catalog.pg_get_userbyid(schema_owner::regrole::oid) AS owner\n",
    "    FROM information_schema.schemata\n",
    "    WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "    ORDER BY schema_name\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\nüìÅ Schemas:\")\n",
    "display(df_schemas)\n",
    "\n",
    "# Get database statistics\n",
    "df_stats = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        current_database() AS database,\n",
    "        pg_size_pretty(pg_database_size(current_database())) AS total_size,\n",
    "        (\n",
    "            SELECT COUNT(*)\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "        ) AS user_tables,\n",
    "        (\n",
    "            SELECT COUNT(*)\n",
    "            FROM information_schema.schemata\n",
    "            WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "        ) AS user_schemas\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\nüìä Database Statistics:\")\n",
    "display(df_stats)\n",
    "\n",
    "conn_target.close()\n",
    "print(\"\\n‚úÖ Inspection complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}