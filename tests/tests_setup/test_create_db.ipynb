{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12320785",
   "metadata": {},
   "source": [
    "# Test Suite: Database Creation and Configuration\n",
    "\n",
    "**Purpose:** Validate the sql_retail_analytics_warehouse database creation and configuration\n",
    "\n",
    "**Scope:**\n",
    "- Database existence and naming\n",
    "- Encoding configuration (UTF-8)\n",
    "- Locale settings (en_GB.UTF-8)\n",
    "- Template configuration\n",
    "- Ownership and privileges\n",
    "- Connection validation\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Existence validation (database created successfully)\n",
    "- Configuration validation (encoding, collation, ctype)\n",
    "- Ownership validation (correct owner assigned)\n",
    "- Connection testing (can establish connections)\n",
    "- Isolation testing (clean template, no extra objects)\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL server running\n",
    "- `setup/create_db.sql` has been executed\n",
    "- Connection credentials available\n",
    "- Required packages: psycopg2, pytest, ipytest, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044c638",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies & Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb006979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import pytest\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "\n",
    "# Configure ipytest for notebook usage\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'postgres',\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'your_password_here')\n",
    "}\n",
    "\n",
    "# Target database name\n",
    "TARGET_DB = 'sql_retail_analytics_warehouse'\n",
    "\n",
    "print(\"✅ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e33715",
   "metadata": {},
   "source": [
    "## Fixtures: Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bb509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixtures defined\n"
     ]
    }
   ],
   "source": [
    "@pytest.fixture(scope='module')\n",
    "def postgres_connection():\n",
    "    \"\"\"Connection to postgres database for catalog queries.\"\"\"\n",
    "    conn = psycopg2.connect(database='postgres', **DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def postgres_cursor(postgres_connection):\n",
    "    \"\"\"Cursor for postgres database.\"\"\"\n",
    "    cursor = postgres_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def target_connection():\n",
    "    \"\"\"Connection to target warehouse database.\"\"\"\n",
    "    conn = psycopg2.connect(database=TARGET_DB, **DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def target_cursor(target_connection):\n",
    "    \"\"\"Cursor for target database.\"\"\"\n",
    "    cursor = target_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "print(\"✅ Fixtures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3610f2e",
   "metadata": {},
   "source": [
    "## Test Suite 1: Database Existence\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_database_exists` - Queries pg_database catalog to verify the database exists\n",
    "2. `test_database_name_exact_match` - Validates database name is exactly 'sql_retail_analytics_warehouse' (case-sensitive)\n",
    "3. `test_database_is_accessible` - Connects to the database and verifies we can execute queries against it\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** executes `SELECT COUNT(*) FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: Count = 1 (database exists exactly once)\n",
    "  - ❌ Failure: Count ≠ 1 (database missing or duplicated)\n",
    "  \n",
    "- **Test 2** executes `SELECT datname FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datname = 'sql_retail_analytics_warehouse' (exact case match)\n",
    "  - ❌ Failure: Name mismatch, wrong case, or NULL\n",
    "  - **Why case matters**: PostgreSQL identifiers are case-sensitive when quoted\n",
    "  \n",
    "- **Test 3** connects directly to the target database and runs `SELECT current_database()`\n",
    "  - ✅ Success: current_database = 'sql_retail_analytics_warehouse'\n",
    "  - ❌ Failure: Connection fails or wrong database name returned\n",
    "  - **Purpose**: Validates not just existence, but actual accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63fcdcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_exists \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 33%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_name_exact_match \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 66%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_is_accessible \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.25s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_exists(postgres_cursor):\n",
    "    \"\"\"Verify sql_retail_analytics_warehouse database exists.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    count = postgres_cursor.fetchone()[0]\n",
    "    assert count == 1, f\"Database '{TARGET_DB}' must exist\"\n",
    "\n",
    "def test_database_name_exact_match(postgres_cursor):\n",
    "    \"\"\"Verify database name matches exactly (case-sensitive).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datname\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    result = postgres_cursor.fetchone()\n",
    "    assert result is not None, f\"Database '{TARGET_DB}' not found\"\n",
    "    assert result[0] == TARGET_DB, f\"Database name mismatch: expected '{TARGET_DB}', got '{result[0]}'\"\n",
    "\n",
    "def test_database_is_accessible(target_cursor):\n",
    "    \"\"\"Verify we can connect to and query the database.\"\"\"\n",
    "    target_cursor.execute(\"SELECT current_database()\")\n",
    "    current_db = target_cursor.fetchone()[0]\n",
    "    assert current_db == TARGET_DB, f\"Connected to wrong database: {current_db}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add5fdc",
   "metadata": {},
   "source": [
    "## Test Suite 2: Encoding Configuration\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_database_encoding_utf8` - Queries pg_database catalog to verify encoding is UTF8\n",
    "2. `test_database_encoding_from_connection` - Uses SHOW command to verify server_encoding setting\n",
    "3. `test_client_encoding_utf8` - Verifies client connection is using UTF8 encoding\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries pg_database catalog: `SELECT pg_encoding_to_char(encoding) FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: encoding = 'UTF8'\n",
    "  - ❌ Failure: encoding ≠ 'UTF8' (e.g., 'LATIN1', 'SQL_ASCII')\n",
    "  - **Purpose**: Validates database was created with UTF8 encoding (critical for international characters)\n",
    "  - **Note**: This is a database creation parameter - cannot be changed without recreating the database\n",
    "  \n",
    "- **Test 2** executes `SHOW server_encoding` within target database connection\n",
    "  - ✅ Success: server_encoding = 'UTF8'\n",
    "  - ❌ Failure: server_encoding ≠ 'UTF8'\n",
    "  - **Purpose**: Cross-validates encoding from runtime perspective\n",
    "  - **Validation approach**: Confirms catalog setting matches active session setting\n",
    "  \n",
    "- **Test 3** executes `SHOW client_encoding` within target database connection\n",
    "  - ✅ Success: client_encoding = 'UTF8'\n",
    "  - ❌ Failure: client_encoding ≠ 'UTF8'\n",
    "  - **Purpose**: Ensures client-server communication uses UTF8 (prevents character corruption)\n",
    "  - **Impact**: Mismatched client/server encodings cause silent data corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5433ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_encoding_utf8 \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 33%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_encoding_from_connection \u001b[32mPASSED\u001b[0m\u001b[32m         [ 66%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_client_encoding_utf8 \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.26s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_encoding_utf8(postgres_cursor):\n",
    "    \"\"\"Verify database uses UTF8 encoding.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT pg_encoding_to_char(encoding) AS encoding\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    encoding = postgres_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Expected UTF8 encoding, got '{encoding}'\"\n",
    "\n",
    "def test_database_encoding_from_connection(target_cursor):\n",
    "    \"\"\"Verify encoding setting from within the database.\"\"\"\n",
    "    target_cursor.execute(\"SHOW server_encoding\")\n",
    "    encoding = target_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Server encoding should be UTF8, got '{encoding}'\"\n",
    "\n",
    "def test_client_encoding_utf8(target_cursor):\n",
    "    \"\"\"Verify client encoding is also UTF8.\"\"\"\n",
    "    target_cursor.execute(\"SHOW client_encoding\")\n",
    "    encoding = target_cursor.fetchone()[0]\n",
    "    assert encoding == 'UTF8', f\"Client encoding should be UTF8, got '{encoding}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97599af8",
   "metadata": {},
   "source": [
    "## Test Suite 3: Locale Configuration\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_database_collation_en_gb` - Verifies datcollate is set to 'en_GB.UTF-8' in pg_database catalog\n",
    "2. `test_database_ctype_en_gb` - Verifies datctype is set to 'en_GB.UTF-8' in pg_database catalog\n",
    "3. `test_lc_collate_matches_database_setting` - Cross-validates LC_COLLATE between postgres and target connections via catalog\n",
    "4. `test_lc_ctype_matches_database_setting` - Cross-validates LC_CTYPE between postgres and target connections via catalog\n",
    "\n",
    "**Note:** LC_COLLATE and LC_CTYPE are database creation parameters stored in pg_database catalog, not runtime session parameters.\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** executes `SELECT datcollate FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datcollate = 'en_GB.UTF-8'\n",
    "  - ❌ Failure: datcollate ≠ 'en_GB.UTF-8' (e.g., 'C', 'en_US.UTF-8')\n",
    "  - **Purpose**: Validates string sorting/comparison rules (affects ORDER BY, indexes, unique constraints)\n",
    "  - **Impact**: Wrong collation can break alphabetical sorting and cause unexpected query results\n",
    "  \n",
    "- **Test 2** executes `SELECT datctype FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datctype = 'en_GB.UTF-8'\n",
    "  - ❌ Failure: datctype ≠ 'en_GB.UTF-8'\n",
    "  - **Purpose**: Validates character classification (uppercase/lowercase conversions, regex, LIKE patterns)\n",
    "  - **Example impact**: UPPER('café') behavior depends on ctype setting\n",
    "  \n",
    "- **Test 3** cross-validates LC_COLLATE consistency across different connection contexts\n",
    "  - From postgres connection: `SELECT datcollate FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - From target connection: `SELECT datcollate FROM pg_database WHERE datname = current_database()`\n",
    "  - ✅ Success: Both queries return identical 'en_GB.UTF-8'\n",
    "  - ❌ Failure: Values don't match (configuration inconsistency)\n",
    "  - **Purpose**: Ensures catalog integrity - same database property should return same value regardless of query source\n",
    "  \n",
    "- **Test 4** cross-validates LC_CTYPE consistency (same approach as Test 3 for datctype)\n",
    "  - ✅ Success: Both queries return identical 'en_GB.UTF-8'\n",
    "  - ❌ Failure: Values don't match\n",
    "  - **Purpose**: Validates ctype configuration is consistent across query contexts\n",
    "\n",
    "**⚠️ CRITICAL: Why these tests matter**\n",
    "- Locale settings are **IMMUTABLE after database creation**\n",
    "- Incorrect settings require **dropping and recreating the entire database**\n",
    "- These tests catch configuration errors **before** any data is loaded\n",
    "- Changing locale after data import causes index corruption and requires full REINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f34b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_collation_en_gb collected 4 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_collation_en_gb \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 25%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_ctype_en_gb \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_lc_collate_matches_database_setting \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 25%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_ctype_en_gb \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_lc_collate_matches_database_setting \u001b[32mPASSED\u001b[0m\u001b[32m       [ 75%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_lc_ctype_matches_database_setting \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m       [ 75%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_lc_ctype_matches_database_setting \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_collation_en_gb(postgres_cursor):\n",
    "    \"\"\"Verify database uses en_GB.UTF-8 collation.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datcollate\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    collation = postgres_cursor.fetchone()[0]\n",
    "    assert collation == 'en_GB.UTF-8', f\"Expected 'en_GB.UTF-8' collation, got '{collation}'\"\n",
    "\n",
    "def test_database_ctype_en_gb(postgres_cursor):\n",
    "    \"\"\"Verify database uses en_GB.UTF-8 character classification.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datctype\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    ctype = postgres_cursor.fetchone()[0]\n",
    "    assert ctype == 'en_GB.UTF-8', f\"Expected 'en_GB.UTF-8' ctype, got '{ctype}'\"\n",
    "\n",
    "def test_lc_collate_matches_database_setting(postgres_cursor, target_cursor):\n",
    "    \"\"\"Verify LC_COLLATE visible in current session matches database configuration.\"\"\"\n",
    "    # Get database collation from catalog\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datcollate\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    expected_collate = postgres_cursor.fetchone()[0]\n",
    "    \n",
    "    # Query via pg_settings or database properties\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT datcollate\n",
    "        FROM pg_database\n",
    "        WHERE datname = current_database()\n",
    "    \"\"\")\n",
    "    actual_collate = target_cursor.fetchone()[0]\n",
    "    \n",
    "    assert actual_collate == expected_collate, \\\n",
    "        f\"LC_COLLATE mismatch: expected '{expected_collate}', got '{actual_collate}'\"\n",
    "\n",
    "def test_lc_ctype_matches_database_setting(postgres_cursor, target_cursor):\n",
    "    \"\"\"Verify LC_CTYPE visible in current session matches database configuration.\"\"\"\n",
    "    # Get database ctype from catalog\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datctype\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    expected_ctype = postgres_cursor.fetchone()[0]\n",
    "    \n",
    "    # Query via database properties\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT datctype\n",
    "        FROM pg_database\n",
    "        WHERE datname = current_database()\n",
    "    \"\"\")\n",
    "    actual_ctype = target_cursor.fetchone()[0]\n",
    "    \n",
    "    assert actual_ctype == expected_ctype, \\\n",
    "        f\"LC_CTYPE mismatch: expected '{expected_ctype}', got '{actual_ctype}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06783f",
   "metadata": {},
   "source": [
    "## Test Suite 4: Template Configuration\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_database_allows_connections` - Verifies datallowconn=true (database accepts connections)\n",
    "2. `test_database_not_a_template` - Verifies datistemplate=false (database is not a template)\n",
    "3. `test_no_active_connections_limit` - Verifies datconnlimit=-1 (unlimited connections allowed)\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** executes `SELECT datallowconn FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datallowconn = True (boolean)\n",
    "  - ❌ Failure: datallowconn = False\n",
    "  - **Purpose**: Ensures database accepts connections (required for data warehouse operations)\n",
    "  - **Context**: Template databases (template0, template1) have datallowconn=False to prevent modifications\n",
    "  \n",
    "- **Test 2** executes `SELECT datistemplate FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datistemplate = False (boolean)\n",
    "  - ❌ Failure: datistemplate = True\n",
    "  - **Purpose**: Confirms this is a working database, not a template\n",
    "  - **Impact**: Template databases cannot be dropped with simple DROP DATABASE command\n",
    "  - **Use case**: Templates are meant for CREATE DATABASE...TEMPLATE operations, not data storage\n",
    "  \n",
    "- **Test 3** executes `SELECT datconnlimit FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: datconnlimit = -1 (integer, unlimited)\n",
    "  - ❌ Failure: datconnlimit ≥ 0 (any specific limit)\n",
    "  - **Purpose**: Validates unlimited concurrent connections (critical for data warehouse with multiple ETL jobs)\n",
    "  - **Value meanings**:\n",
    "    - `-1` = unlimited connections (required for production warehouses)\n",
    "    - `0` = no connections allowed (database locked)\n",
    "    - Positive number = maximum concurrent connections\n",
    "\n",
    "**💼 Business Impact:**\n",
    "Unlimited connections (-1) are essential for data warehouses where:\n",
    "- Multiple ETL processes run simultaneously\n",
    "- BI tools maintain connection pools\n",
    "- Analysts query data concurrently\n",
    "- Monitoring tools maintain persistent connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f626a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_allows_connections collected 3 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_allows_connections \u001b[32mPASSED\u001b[0m\u001b[32m               [ 33%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_not_a_template \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 66%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_no_active_connections_limit \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m               [ 33%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_not_a_template \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 66%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_no_active_connections_limit \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_allows_connections(postgres_cursor):\n",
    "    \"\"\"Verify database allows connections (not a template).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datallowconn\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    allows_conn = postgres_cursor.fetchone()[0]\n",
    "    assert allows_conn is True, \"Database should allow connections\"\n",
    "\n",
    "def test_database_not_a_template(postgres_cursor):\n",
    "    \"\"\"Verify database is not marked as a template.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datistemplate\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    is_template = postgres_cursor.fetchone()[0]\n",
    "    assert is_template is False, \"Database should not be a template\"\n",
    "\n",
    "def test_no_active_connections_limit(postgres_cursor):\n",
    "    \"\"\"Verify database has no connection limit (-1 = unlimited).\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT datconnlimit\n",
    "        FROM pg_database\n",
    "        WHERE datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    conn_limit = postgres_cursor.fetchone()[0]\n",
    "    assert conn_limit == -1, f\"Expected unlimited connections (-1), got {conn_limit}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de06c8e",
   "metadata": {},
   "source": [
    "## Test Suite 5: Ownership and Privileges\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_database_owner` - Queries pg_database to verify database has a valid owner (postgres or creating user)\n",
    "2. `test_current_user_can_create_schema` - Creates a test schema to verify CREATE SCHEMA privilege, then cleans up\n",
    "\n",
    "**Note:** Tests verify sufficient privileges for data warehouse operations.\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries database ownership: `SELECT pg_catalog.pg_get_userbyid(d.datdba) AS owner FROM pg_catalog.pg_database d WHERE d.datname = 'sql_retail_analytics_warehouse'`\n",
    "  - ✅ Success: owner = 'postgres' (or creating user name, non-empty string)\n",
    "  - ❌ Failure: owner is NULL or empty string\n",
    "  - **Purpose**: Validates database has a valid owner with full privileges\n",
    "  - **Technical note**: `pg_get_userbyid()` converts internal OID (Object Identifier) to human-readable username\n",
    "  - **Security**: Database owner has unrestricted access to all objects and can grant privileges to others\n",
    "  \n",
    "- **Test 2** performs a CREATE/DROP cycle to test DDL privileges:\n",
    "  1. Executes `CREATE SCHEMA IF NOT EXISTS test_privilege_check`\n",
    "  2. Verifies creation: `SELECT COUNT(*) FROM information_schema.schemata WHERE schema_name = 'test_privilege_check'`\n",
    "     - ✅ Success: count = 1 (schema created successfully)\n",
    "     - ❌ Failure: count ≠ 1 or SQL error (insufficient privileges)\n",
    "  3. Cleans up: `DROP SCHEMA IF EXISTS test_privilege_check CASCADE`\n",
    "  - **Purpose**: Validates current user can perform essential DDL operations (required for Bronze/Silver/Gold schema creation)\n",
    "  - **Side effects**: Creates and removes test_privilege_check schema (ephemeral, no lasting impact)\n",
    "  - **Why functional testing**: Checking pg_catalog for privileges doesn't guarantee actual DDL operations will succeed\n",
    "\n",
    "**⚠️ Why This Matters:**\n",
    "- Without CREATE SCHEMA privilege, the entire medallion architecture setup (Bronze/Silver/Gold layers) would fail\n",
    "- This test catches permission issues **before** running expensive ETL scripts\n",
    "- Functional privilege testing is more reliable than catalog-based permission checks\n",
    "- Prevents partial setup failures that are difficult to rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42995fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_owner collected 2 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_owner \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_current_user_can_create_schema \u001b[32mPASSED\u001b[0m\u001b[32m                            [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_current_user_can_create_schema \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_database_owner(postgres_cursor):\n",
    "    \"\"\"Verify database is owned by postgres user.\"\"\"\n",
    "    postgres_cursor.execute(\"\"\"\n",
    "        SELECT pg_catalog.pg_get_userbyid(d.datdba) AS owner\n",
    "        FROM pg_catalog.pg_database d\n",
    "        WHERE d.datname = %s\n",
    "    \"\"\", (TARGET_DB,))\n",
    "    \n",
    "    owner = postgres_cursor.fetchone()[0]\n",
    "    # Owner should be postgres or the creating user\n",
    "    assert owner is not None, \"Database must have an owner\"\n",
    "    assert len(owner) > 0, \"Owner name should not be empty\"\n",
    "\n",
    "def test_current_user_can_create_schema(target_cursor):\n",
    "    \"\"\"Verify current user has privileges to create schemas.\"\"\"\n",
    "    # Try to create a test schema (will rollback)\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS test_privilege_check\n",
    "    \"\"\")\n",
    "    \n",
    "    # Verify it was created\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.schemata\n",
    "        WHERE schema_name = 'test_privilege_check'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = target_cursor.fetchone()[0]\n",
    "    assert count == 1, \"User should be able to create schemas\"\n",
    "    \n",
    "    # Clean up\n",
    "    target_cursor.execute(\"DROP SCHEMA IF EXISTS test_privilege_check CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b8675",
   "metadata": {},
   "source": [
    "## Test Suite 6: Clean State Validation\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_expected_default_schemas_only` - Validates only expected schemas exist (public, bronze, silver, gold, setup)\n",
    "2. `test_database_size_reasonable` - Checks database size is under 20MB (reasonable for a clean warehouse)\n",
    "\n",
    "**Note:** These tests ensure the database starts in a clean, predictable state for the data warehouse.\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** queries all user-created schemas: `SELECT schema_name FROM information_schema.schemata WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast') ORDER BY schema_name`\n",
    "  - ✅ Success: schemas in {'public', 'bronze', 'silver', 'gold', 'setup'} (after create_schemas.sql runs)\n",
    "  - ⚠️ Warning: Prints list if unexpected schemas found (e.g., 'test', 'temp', 'legacy')\n",
    "  - ❌ Failure: Only fails if 'public' schema is missing (always required)\n",
    "  - **Purpose**: Detects schema pollution or incomplete cleanup from previous operations\n",
    "  - **Behavior**: Lenient approach - warns but doesn't fail on extra schemas (allows for dev experimentation)\n",
    "  - **Filtered schemas**: Excludes PostgreSQL system schemas (pg_catalog, information_schema, pg_toast)\n",
    "  \n",
    "- **Test 2** queries database size:\n",
    "  ```sql\n",
    "  SELECT pg_size_pretty(pg_database_size(current_database())) AS size,\n",
    "         pg_database_size(current_database()) AS size_bytes\n",
    "  ```\n",
    "  - ✅ Success: size_bytes < 20,971,520 (20MB)\n",
    "  - ❌ Failure: size_bytes ≥ 20MB (indicates data/objects already loaded)\n",
    "  - **Typical clean database size**: 8-10MB (includes system catalogs and empty schemas)\n",
    "  - **Purpose**: Ensures database is in initial clean state before ETL operations begin\n",
    "  - **Diagnostic**: Always prints human-readable size (e.g., \"9845 kB\") for visual confirmation\n",
    "  - **Size components**: Includes all tables, indexes, sequences, and system objects\n",
    "\n",
    "**🔍 Troubleshooting Failed Tests:**\n",
    "- **Unexpected schemas found**: \n",
    "  - May indicate incomplete cleanup from previous testing\n",
    "  - Could be leftover from failed ETL runs\n",
    "  - Action: Review schema list, DROP unwanted schemas if safe\n",
    "  \n",
    "- **Size > 20MB**: \n",
    "  - Database may already have data loaded (not a clean state)\n",
    "  - Large objects (LOBs, BLOBs) may exist\n",
    "  - Bloated system catalogs from many DROP/CREATE cycles\n",
    "  - Action: Investigate with `SELECT pg_size_pretty(pg_total_relation_size('schema.table'))` on largest tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a46118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_expected_default_schemas_only collected 2 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_expected_default_schemas_only \u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_size_reasonable \u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_database_size_reasonable \u001b[32mPASSED\u001b[0m\u001b[32m                  [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_expected_default_schemas_only(target_cursor):\n",
    "    \"\"\"Verify only default PostgreSQL schemas exist (if using template0).\"\"\"\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT schema_name\n",
    "        FROM information_schema.schemata\n",
    "        WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "        ORDER BY schema_name\n",
    "    \"\"\")\n",
    "    \n",
    "    schemas = [row[0] for row in target_cursor.fetchall()]\n",
    "    \n",
    "    # Should only have 'public' by default (if clean template0)\n",
    "    # May have bronze/silver/gold if create_schemas.sql was run\n",
    "    # This test verifies no unexpected schemas exist\n",
    "    expected_schemas = {'public', 'bronze', 'silver', 'gold', 'setup'}\n",
    "    unexpected = set(schemas) - expected_schemas\n",
    "    \n",
    "    if unexpected:\n",
    "        print(f\"⚠️  Unexpected schemas found: {unexpected}\")\n",
    "        print(f\"   All schemas: {schemas}\")\n",
    "    \n",
    "    # At minimum, 'public' should exist\n",
    "    assert 'public' in schemas, \"Default 'public' schema should exist\"\n",
    "\n",
    "def test_database_size_reasonable(target_cursor):\n",
    "    \"\"\"Verify database size is reasonable for a clean database.\"\"\"\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT pg_size_pretty(pg_database_size(current_database())) AS size,\n",
    "               pg_database_size(current_database()) AS size_bytes\n",
    "    \"\"\")\n",
    "    \n",
    "    size_pretty, size_bytes = target_cursor.fetchone()\n",
    "    \n",
    "    # Clean database should be under 20MB\n",
    "    max_size_bytes = 20 * 1024 * 1024  # 20MB\n",
    "    \n",
    "    print(f\"Database size: {size_pretty}\")\n",
    "    assert size_bytes < max_size_bytes, \\\n",
    "        f\"Database seems too large for a clean database: {size_pretty}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8ec37",
   "metadata": {},
   "source": [
    "## Test Suite 7: Connection and Session Settings\n",
    "\n",
    "**Tests in this suite:**\n",
    "1. `test_timezone_setting` - Verifies timezone is configured (uses SHOW timezone)\n",
    "2. `test_datestyle_setting` - Verifies DateStyle includes 'ISO' for consistent date formatting\n",
    "3. `test_can_create_table` - Creates and drops a test table to validate DDL operations work\n",
    "4. `test_can_insert_and_query_utf8_data` - Inserts and retrieves UTF-8 data (emojis, Japanese, Greek) to verify encoding works correctly\n",
    "\n",
    "**Note:** These tests validate the database is fully functional and ready for data warehouse operations.\n",
    "\n",
    "**How these tests work:**\n",
    "- **Test 1** executes `SHOW timezone`\n",
    "  - ✅ Success: Non-empty string (e.g., 'UTC', 'Europe/London', 'America/New_York')\n",
    "  - ❌ Failure: timezone is NULL or empty string\n",
    "  - **Purpose**: Validates timezone configuration exists (critical for timestamp operations)\n",
    "  - **Note**: Actual timezone value not tested (depends on server/client configuration)\n",
    "  - **Impact**: Affects TIMESTAMP WITH TIME ZONE storage and display\n",
    "  \n",
    "- **Test 2** executes `SHOW datestyle`\n",
    "  - ✅ Success: String containing 'ISO' (e.g., 'ISO, DMY', 'ISO, MDY')\n",
    "  - ❌ Failure: 'ISO' not in datestyle string (e.g., 'Postgres, DMY', 'SQL, MDY')\n",
    "  - **Purpose**: Ensures consistent date formatting (ISO standard prevents ambiguity)\n",
    "  - **Why ISO matters**: \n",
    "    - ISO format (YYYY-MM-DD) is unambiguous internationally\n",
    "    - 'Postgres' style (Mon DD YYYY) varies by locale\n",
    "    - 'SQL' style varies by regional settings\n",
    "  - **Data warehouse impact**: Prevents date parsing errors in ETL pipelines\n",
    "  \n",
    "- **Test 3** performs full DDL cycle to validate table operations:\n",
    "  1. Executes `CREATE TABLE IF NOT EXISTS test_table_creation (id SERIAL PRIMARY KEY, name TEXT NOT NULL)`\n",
    "  2. Verifies: `SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'test_table_creation'`\n",
    "     - ✅ Success: count = 1\n",
    "     - ❌ Failure: count ≠ 1 or SQL error\n",
    "  3. Cleans up: `DROP TABLE IF EXISTS test_table_creation`\n",
    "  - **Purpose**: Validates DDL operations work (required for Bronze layer table creation)\n",
    "  - **Why functional testing**: Tests actual CREATE TABLE capability, not just theoretical permissions\n",
    "  - **Side effects**: Temporarily creates and removes test_table_creation (no lasting impact)\n",
    "  - **Tests**: PRIMARY KEY constraint creation, SERIAL sequence generation, NOT NULL constraints\n",
    "  \n",
    "- **Test 4** performs comprehensive UTF-8 validation:\n",
    "  1. Creates temp table: `CREATE TEMP TABLE test_utf8 (data TEXT)`\n",
    "  2. Inserts 5 diverse test strings:\n",
    "     - 'Hello World' (ASCII baseline - control case)\n",
    "     - 'Café' (Latin-1 Supplement: é = U+00E9)\n",
    "     - '日本語' (Japanese CJK Unified Ideographs)\n",
    "     - '🎯📊' (Emoji: U+1F3AF, U+1F4CA - 4-byte UTF-8)\n",
    "     - 'Ελληνικά' (Greek Extended characters)\n",
    "  3. Queries back: `SELECT data FROM test_utf8 ORDER BY data`\n",
    "  4. Validates: len(results) == 5 (all strings retrievable without corruption)\n",
    "  - ✅ Success: All 5 strings returned exactly as inserted\n",
    "  - ❌ Failure: Count ≠ 5, or any string corrupted (e.g., '???' or '日???')\n",
    "  - **Purpose**: End-to-end UTF-8 validation (client → network → storage → retrieval)\n",
    "  - **Technical**: Uses TEMP table (automatically cleaned up at session end, no cleanup code needed)\n",
    "  - **Coverage**: Tests 1-byte (ASCII), 2-byte (Latin), 3-byte (CJK), and 4-byte (Emoji) UTF-8 sequences\n",
    "\n",
    "**🌍 Why UTF-8 Testing is Critical:**\n",
    "- Retail data includes international customer names (e.g., 'François', 'José', '田中')\n",
    "- Product descriptions in multiple languages\n",
    "- Special characters in addresses, currencies (€, £, ¥)\n",
    "- Modern data includes emojis in customer feedback, social media\n",
    "- **UTF-8 corruption is irreversible** - once data is corrupted, original values are lost\n",
    "- Silent corruption is worse than errors - bad data looks valid but is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd0f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.12.4, pytest-8.4.2, pluggy-1.6.0 -- c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\.venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\Laurent\\Studies\\sql-ultimate-course\\Udemy-SQL-Data-Warehouse-Project\\tests\\tests_setup\n",
      "plugins: anyio-4.11.0, nbmake-1.5.5\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_timezone_setting collected 4 items\n",
      "\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_timezone_setting \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 25%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_datestyle_setting \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_can_create_table \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 25%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_datestyle_setting \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_can_create_table \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 75%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_can_insert_and_query_utf8_data \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 75%]\u001b[0m\n",
      "t_2cd27d8b688c4eb9be5163904688617e.py::test_can_insert_and_query_utf8_data \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ========================================\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_timezone_setting(target_cursor):\n",
    "    \"\"\"Verify timezone is set (should have default or configured value).\"\"\"\n",
    "    target_cursor.execute(\"SHOW timezone\")\n",
    "    timezone = target_cursor.fetchone()[0]\n",
    "    assert timezone is not None, \"Timezone should be set\"\n",
    "    assert len(timezone) > 0, \"Timezone value should not be empty\"\n",
    "\n",
    "def test_datestyle_setting(target_cursor):\n",
    "    \"\"\"Verify DateStyle is set to ISO standard.\"\"\"\n",
    "    target_cursor.execute(\"SHOW datestyle\")\n",
    "    datestyle = target_cursor.fetchone()[0]\n",
    "    # Should contain 'ISO' for consistent date formatting\n",
    "    assert 'ISO' in datestyle, f\"DateStyle should include ISO, got '{datestyle}'\"\n",
    "\n",
    "def test_can_create_table(target_cursor):\n",
    "    \"\"\"Verify basic DDL operations work.\"\"\"\n",
    "    # Create a test table\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS test_table_creation (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name TEXT NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Verify it exists\n",
    "    target_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name = 'test_table_creation'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = target_cursor.fetchone()[0]\n",
    "    assert count == 1, \"Should be able to create tables\"\n",
    "    \n",
    "    # Clean up\n",
    "    target_cursor.execute(\"DROP TABLE IF EXISTS test_table_creation\")\n",
    "\n",
    "def test_can_insert_and_query_utf8_data(target_cursor):\n",
    "    \"\"\"Verify UTF-8 data can be inserted and queried correctly.\"\"\"\n",
    "    # Create temp table\n",
    "    target_cursor.execute(\"\"\"\n",
    "        CREATE TEMP TABLE test_utf8 (data TEXT)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert various UTF-8 characters\n",
    "    test_strings = [\n",
    "        'Hello World',\n",
    "        'Café',\n",
    "        '日本語',  # Japanese\n",
    "        '🎯📊',    # Emojis\n",
    "        'Ελληνικά'  # Greek\n",
    "    ]\n",
    "    \n",
    "    for test_str in test_strings:\n",
    "        target_cursor.execute(\n",
    "            \"INSERT INTO test_utf8 (data) VALUES (%s)\",\n",
    "            (test_str,)\n",
    "        )\n",
    "    \n",
    "    # Query back and verify\n",
    "    target_cursor.execute(\"SELECT data FROM test_utf8 ORDER BY data\")\n",
    "    results = [row[0] for row in target_cursor.fetchall()]\n",
    "    \n",
    "    # All strings should be retrievable\n",
    "    assert len(results) == len(test_strings), \"All UTF-8 strings should be retrievable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419b624",
   "metadata": {},
   "source": [
    "## Summary: Run All Tests\n",
    "\n",
    "**Executes all test suites (22 tests total):**\n",
    "- Suite 1: Database Existence (3 tests)\n",
    "- Suite 2: Encoding Configuration (3 tests)\n",
    "- Suite 3: Locale Configuration (4 tests)\n",
    "- Suite 4: Template Configuration (3 tests)\n",
    "- Suite 5: Ownership and Privileges (2 tests)\n",
    "- Suite 6: Clean State Validation (2 tests)\n",
    "- Suite 7: Connection and Session Settings (4 tests)\n",
    "\n",
    "**How this cell works:**\n",
    "- Executes `ipytest.run('-vv')` which runs all pytest functions defined in this notebook\n",
    "- `-vv` flag provides **very verbose** output showing:\n",
    "  - Each test function name as it runs\n",
    "  - PASSED/FAILED status for each test\n",
    "  - Detailed assertion messages on failure\n",
    "  - Full traceback on errors\n",
    "  - Percentage completion progress\n",
    "  \n",
    "**What verbose output includes:**\n",
    "- Test collection phase (shows how many tests found)\n",
    "- Platform and Python version info\n",
    "- Pytest version and loaded plugins\n",
    "- Individual test results with pass/fail status\n",
    "- Captured stdout/stderr for failed tests\n",
    "- Final summary with counts and execution time\n",
    "\n",
    "**✅ Success Criteria:**\n",
    "- All 22 tests show `PASSED` status\n",
    "- Final summary shows: `22 passed in X.XXs`\n",
    "- No `FAILED`, `ERROR`, or `SKIPPED` statuses\n",
    "- No warnings about connection issues or deprecations\n",
    "\n",
    "**🔧 Troubleshooting Test Failures:**\n",
    "\n",
    "| Failure Type | Likely Cause | Solution |\n",
    "|-------------|--------------|----------|\n",
    "| Connection refused | PostgreSQL not running | Start PostgreSQL service |\n",
    "| Password authentication failed | Wrong `.env` credentials | Verify POSTGRES_PASSWORD in `.env` |\n",
    "| Database does not exist | `create_db.sql` not run | Execute `setup/create_db.sql` first |\n",
    "| Encoding mismatch | Database created with wrong encoding | Drop and recreate with `ENCODING='UTF8'` |\n",
    "| Locale mismatch | Database created with wrong locale | Drop and recreate with correct LC_COLLATE/LC_CTYPE |\n",
    "| Permission denied | Insufficient user privileges | Grant CREATE privilege to current user |\n",
    "| Size test fails | Data already loaded | Drop and recreate database for clean state |\n",
    "\n",
    "**📊 Reading Test Output:**\n",
    "- `[  4%]` indicates progress through test suite\n",
    "- `PASSED` in green = test succeeded\n",
    "- `FAILED` in red = assertion failed (see details below)\n",
    "- `ERROR` in red = test couldn't run (setup issue)\n",
    "- Final line shows total time - useful for performance tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests in this notebook\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6d313",
   "metadata": {},
   "source": [
    "## Manual Inspection: Database Properties\n",
    "\n",
    "**What this cell does:**\n",
    "1. **Database Configuration** - Displays all database properties from pg_database catalog (owner, encoding, collation, ctype, size, etc.)\n",
    "2. **Session Settings** - Shows runtime configuration parameters (server_encoding, client_encoding, timezone, datestyle)\n",
    "3. **Locale Settings** - Displays database creation locale parameters (lc_collate, lc_ctype) from catalog\n",
    "4. **Schemas** - Lists all user-created schemas with their owners\n",
    "5. **Database Statistics** - Shows database size, table count, and schema count\n",
    "\n",
    "Run this cell for a comprehensive visual overview of the database configuration.\n",
    "\n",
    "**How this cell works:**\n",
    "\n",
    "**Step 1: Database Configuration (pg_database catalog query)**\n",
    "```sql\n",
    "SELECT\n",
    "    d.datname,                                  -- Database name\n",
    "    pg_catalog.pg_get_userbyid(d.datdba),      -- Owner (converts OID to username)\n",
    "    pg_catalog.pg_encoding_to_char(d.encoding), -- Encoding (e.g., UTF8)\n",
    "    d.datcollate,                              -- Collation (e.g., en_GB.UTF-8)\n",
    "    d.datctype,                                -- Character classification\n",
    "    d.datallowconn,                            -- Allows connections (boolean)\n",
    "    d.datconnlimit,                            -- Connection limit (-1 = unlimited)\n",
    "    d.datistemplate,                           -- Is template (boolean)\n",
    "    pg_size_pretty(pg_database_size(d.datname)) -- Human-readable size\n",
    "FROM pg_catalog.pg_database d\n",
    "WHERE d.datname = 'sql_retail_analytics_warehouse'\n",
    "```\n",
    "- **Output format**: Transposed DataFrame (properties as rows for readability)\n",
    "- **Expected values**: See test suites above for expected configuration\n",
    "\n",
    "**Step 2: Session Settings (runtime parameters)**\n",
    "```sql\n",
    "SELECT 'server_encoding', current_setting('server_encoding')\n",
    "UNION ALL SELECT 'client_encoding', current_setting('client_encoding')\n",
    "UNION ALL SELECT 'timezone', current_setting('timezone')\n",
    "UNION ALL SELECT 'datestyle', current_setting('datestyle')\n",
    "```\n",
    "- **Output format**: 2-column DataFrame (setting | value)\n",
    "- **Expected values**:\n",
    "  - server_encoding: 'UTF8'\n",
    "  - client_encoding: 'UTF8'\n",
    "  - timezone: System dependent (e.g., 'UTC', 'Europe/London')\n",
    "  - datestyle: Should include 'ISO'\n",
    "\n",
    "**Step 3: Locale Settings (database catalog - not runtime)**\n",
    "```sql\n",
    "SELECT 'lc_collate', datcollate FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'\n",
    "UNION ALL\n",
    "SELECT 'lc_ctype', datctype FROM pg_database WHERE datname = 'sql_retail_analytics_warehouse'\n",
    "```\n",
    "- **Output format**: 2-column DataFrame (setting | value)\n",
    "- **Expected values**:\n",
    "  - lc_collate: 'en_GB.UTF-8'\n",
    "  - lc_ctype: 'en_GB.UTF-8'\n",
    "- **Note**: Cannot use current_setting() - these are database creation parameters only\n",
    "\n",
    "**Step 4: Schemas (user-created schemas)**\n",
    "```sql\n",
    "SELECT\n",
    "    schema_name,\n",
    "    pg_catalog.pg_get_userbyid(schema_owner::regrole::oid) AS owner\n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "ORDER BY schema_name\n",
    "```\n",
    "- **Output format**: 2-column DataFrame (schema_name | owner)\n",
    "- **Expected values**: {public, bronze, silver, gold, setup} with their respective owners\n",
    "\n",
    "**Step 5: Database Statistics (size and object counts)**\n",
    "```sql\n",
    "SELECT\n",
    "    current_database() AS database,\n",
    "    pg_size_pretty(pg_database_size(current_database())) AS total_size,\n",
    "    (SELECT COUNT(*) FROM information_schema.tables \n",
    "     WHERE table_schema NOT IN ('pg_catalog', 'information_schema')) AS user_tables,\n",
    "    (SELECT COUNT(*) FROM information_schema.schemata \n",
    "     WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')) AS user_schemas\n",
    "```\n",
    "- **Output format**: Single-row DataFrame with 4 columns\n",
    "- **Expected values**:\n",
    "  - database: 'sql_retail_analytics_warehouse'\n",
    "  - total_size: < 20MB (e.g., \"9845 kB\")\n",
    "  - user_tables: Depends on setup stage (0 if clean, >0 if Bronze tables created)\n",
    "  - user_schemas: 5 (public, bronze, silver, gold, setup)\n",
    "\n",
    "**Expected console output:**\n",
    "```\n",
    "🗄️  Database Configuration:\n",
    "[Transposed DataFrame showing all database properties]\n",
    "\n",
    "⚙️  Session Settings:\n",
    "[DataFrame with 4 runtime settings]\n",
    "\n",
    "🌍 Locale Settings (from database catalog):\n",
    "[DataFrame with 2 locale settings]\n",
    "\n",
    "📁 Schemas:\n",
    "[DataFrame listing all user schemas]\n",
    "\n",
    "📊 Database Statistics:\n",
    "[Single-row DataFrame with size and counts]\n",
    "\n",
    "✅ Inspection complete\n",
    "```\n",
    "\n",
    "**Use this for:**\n",
    "- Visual confirmation of all test assertions\n",
    "- Debugging test failures\n",
    "- Documentation/screenshots of database configuration\n",
    "- Comparing expected vs. actual configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc4c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗄️  Database Configuration:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_6856\\2333232674.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_db_info = pd.read_sql(f\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "cc8c250b-4898-4a9e-90ff-d108fd760e3d",
       "rows": [
        [
         "database_name",
         "sql_retail_analytics_warehouse"
        ],
        [
         "owner",
         "postgres"
        ],
        [
         "encoding",
         "UTF8"
        ],
        [
         "collation",
         "en_GB.UTF-8"
        ],
        [
         "ctype",
         "en_GB.UTF-8"
        ],
        [
         "allows_connections",
         "True"
        ],
        [
         "connection_limit",
         "-1"
        ],
        [
         "is_template",
         "False"
        ],
        [
         "size",
         "8062 kB"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>database_name</th>\n",
       "      <td>sql_retail_analytics_warehouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owner</th>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encoding</th>\n",
       "      <td>UTF8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collation</th>\n",
       "      <td>en_GB.UTF-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctype</th>\n",
       "      <td>en_GB.UTF-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allows_connections</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connection_limit</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_template</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>8062 kB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0\n",
       "database_name       sql_retail_analytics_warehouse\n",
       "owner                                     postgres\n",
       "encoding                                      UTF8\n",
       "collation                              en_GB.UTF-8\n",
       "ctype                                  en_GB.UTF-8\n",
       "allows_connections                            True\n",
       "connection_limit                                -1\n",
       "is_template                                  False\n",
       "size                                       8062 kB"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️  Session Settings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_6856\\2333232674.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_settings = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "setting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "value",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "67187289-1643-4840-8240-dd9e2b659613",
       "rows": [
        [
         "0",
         "client_encoding",
         "UTF8"
        ],
        [
         "1",
         "datestyle",
         "ISO, DMY"
        ],
        [
         "2",
         "server_encoding",
         "UTF8"
        ],
        [
         "3",
         "timezone",
         "Europe/London"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_encoding</td>\n",
       "      <td>UTF8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datestyle</td>\n",
       "      <td>ISO, DMY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>server_encoding</td>\n",
       "      <td>UTF8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timezone</td>\n",
       "      <td>Europe/London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           setting          value\n",
       "0  client_encoding           UTF8\n",
       "1        datestyle       ISO, DMY\n",
       "2  server_encoding           UTF8\n",
       "3         timezone  Europe/London"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌍 Locale Settings (from database catalog):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_6856\\2333232674.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_locale = pd.read_sql(f\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "setting",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "value",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0b1e314d-1999-494d-85c9-d8002762a901",
       "rows": [
        [
         "0",
         "lc_collate",
         "en_GB.UTF-8"
        ],
        [
         "1",
         "lc_ctype",
         "en_GB.UTF-8"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lc_collate</td>\n",
       "      <td>en_GB.UTF-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lc_ctype</td>\n",
       "      <td>en_GB.UTF-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      setting        value\n",
       "0  lc_collate  en_GB.UTF-8\n",
       "1    lc_ctype  en_GB.UTF-8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Schemas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_6856\\2333232674.py:65: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_schemas = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "schema_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "owner",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "638ed19e-575a-4148-a655-7bef6b15e25f",
       "rows": [
        [
         "0",
         "bronze",
         "postgres"
        ],
        [
         "1",
         "gold",
         "postgres"
        ],
        [
         "2",
         "pg_temp_28",
         "postgres"
        ],
        [
         "3",
         "pg_toast_temp_28",
         "postgres"
        ],
        [
         "4",
         "public",
         "pg_database_owner"
        ],
        [
         "5",
         "silver",
         "postgres"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schema_name</th>\n",
       "      <th>owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bronze</td>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gold</td>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pg_temp_28</td>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pg_toast_temp_28</td>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public</td>\n",
       "      <td>pg_database_owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>silver</td>\n",
       "      <td>postgres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        schema_name              owner\n",
       "0            bronze           postgres\n",
       "1              gold           postgres\n",
       "2        pg_temp_28           postgres\n",
       "3  pg_toast_temp_28           postgres\n",
       "4            public  pg_database_owner\n",
       "5            silver           postgres"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Database Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\AppData\\Local\\Temp\\ipykernel_6856\\2333232674.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_stats = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "database",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "total_size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user_tables",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "user_schemas",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a774ebd5-6405-4d91-85de-94bcddef5b05",
       "rows": [
        [
         "0",
         "sql_retail_analytics_warehouse",
         "8062 kB",
         "0",
         "6"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>total_size</th>\n",
       "      <th>user_tables</th>\n",
       "      <th>user_schemas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sql_retail_analytics_warehouse</td>\n",
       "      <td>8062 kB</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         database total_size  user_tables  user_schemas\n",
       "0  sql_retail_analytics_warehouse    8062 kB            0             6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Inspection complete\n"
     ]
    }
   ],
   "source": [
    "# Connect to postgres database to query catalog\n",
    "conn_postgres = psycopg2.connect(database='postgres', **DB_CONFIG)\n",
    "\n",
    "# Get comprehensive database information\n",
    "df_db_info = pd.read_sql(f\"\"\"\n",
    "    SELECT\n",
    "        d.datname                                  AS database_name,\n",
    "        pg_catalog.pg_get_userbyid(d.datdba)       AS owner,\n",
    "        pg_catalog.pg_encoding_to_char(d.encoding) AS encoding,\n",
    "        d.datcollate                               AS collation,\n",
    "        d.datctype                                 AS ctype,\n",
    "        d.datallowconn                             AS allows_connections,\n",
    "        d.datconnlimit                             AS connection_limit,\n",
    "        d.datistemplate                            AS is_template,\n",
    "        pg_size_pretty(pg_database_size(d.datname)) AS size\n",
    "    FROM pg_catalog.pg_database d\n",
    "    WHERE d.datname = '{TARGET_DB}'\n",
    "\"\"\", conn_postgres)\n",
    "\n",
    "print(\"\\n🗄️  Database Configuration:\")\n",
    "display(df_db_info.T)  # Transpose for better readability\n",
    "\n",
    "conn_postgres.close()\n",
    "\n",
    "# Connect to target database for additional info\n",
    "conn_target = psycopg2.connect(database=TARGET_DB, **DB_CONFIG)\n",
    "\n",
    "# Get session settings (excluding lc_collate and lc_ctype which are not runtime parameters)\n",
    "df_settings = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        'server_encoding' AS setting,\n",
    "        current_setting('server_encoding') AS value\n",
    "    UNION ALL\n",
    "    SELECT 'client_encoding', current_setting('client_encoding')\n",
    "    UNION ALL\n",
    "    SELECT 'timezone', current_setting('timezone')\n",
    "    UNION ALL\n",
    "    SELECT 'datestyle', current_setting('datestyle')\n",
    "    ORDER BY setting\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\n⚙️  Session Settings:\")\n",
    "display(df_settings)\n",
    "\n",
    "# Get locale settings from database catalog (not runtime parameters)\n",
    "df_locale = pd.read_sql(f\"\"\"\n",
    "    SELECT\n",
    "        'lc_collate' AS setting,\n",
    "        datcollate AS value\n",
    "    FROM pg_database\n",
    "    WHERE datname = '{TARGET_DB}'\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'lc_ctype' AS setting,\n",
    "        datctype AS value\n",
    "    FROM pg_database\n",
    "    WHERE datname = '{TARGET_DB}'\n",
    "    ORDER BY setting\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\n🌍 Locale Settings (from database catalog):\")\n",
    "display(df_locale)\n",
    "\n",
    "# Get list of schemas\n",
    "df_schemas = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        schema_name,\n",
    "        pg_catalog.pg_get_userbyid(schema_owner::regrole::oid) AS owner\n",
    "    FROM information_schema.schemata\n",
    "    WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "    ORDER BY schema_name\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\n📁 Schemas:\")\n",
    "display(df_schemas)\n",
    "\n",
    "# Get database statistics\n",
    "df_stats = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        current_database() AS database,\n",
    "        pg_size_pretty(pg_database_size(current_database())) AS total_size,\n",
    "        (\n",
    "            SELECT COUNT(*)\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "        ) AS user_tables,\n",
    "        (\n",
    "            SELECT COUNT(*)\n",
    "            FROM information_schema.schemata\n",
    "            WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast')\n",
    "        ) AS user_schemas\n",
    "\"\"\", conn_target)\n",
    "\n",
    "print(\"\\n📊 Database Statistics:\")\n",
    "display(df_stats)\n",
    "\n",
    "conn_target.close()\n",
    "print(\"\\n✅ Inspection complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
