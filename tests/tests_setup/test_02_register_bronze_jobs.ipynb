{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827785b0",
   "metadata": {},
   "source": [
    "# Test Suite: Bronze Job Registration and Load Manifest\n",
    "\n",
    "**Purpose:** Validate bronze.load_jobs metadata registry population and configuration\n",
    "\n",
    "**Scope:**\n",
    "- Job registration completeness (all bronze tables registered)\n",
    "- File path correctness (proper formatting, no trailing slashes)\n",
    "- Load order sequencing (CRM before ERP)\n",
    "- Table discovery accuracy\n",
    "- Configuration dependency validation\n",
    "- Idempotency verification\n",
    "\n",
    "**Testing Strategy:**\n",
    "- Registration validation (all tables have job records)\n",
    "- Path validation (correct base paths, proper CSV extensions)\n",
    "- Order validation (CRM = 0-999, ERP = 1000+)\n",
    "- Naming convention validation (source_dataset pattern)\n",
    "- Configuration validation (etl_config values used correctly)\n",
    "- Idempotency validation (re-runs don't create duplicates)\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL server running\n",
    "- sql_retail_analytics_warehouse database exists\n",
    "- bronze schema exists\n",
    "- `setup/seed/02_register_bronze_jobs.sql` has been executed\n",
    "- Bronze tables created (crm_*, erp_*)\n",
    "- Connection credentials available\n",
    "- Required packages: psycopg2, pytest, ipytest, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356635d",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies & Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pytest\n",
    "import ipytest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Configure ipytest for notebook usage\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'sql_retail_analytics_warehouse',\n",
    "    'user': 'postgres',\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'your_password_here')\n",
    "}\n",
    "\n",
    "# Expected source systems\n",
    "EXPECTED_SOURCES = ['crm', 'erp']\n",
    "\n",
    "# Load order boundaries\n",
    "CRM_MAX_ORDER = 999\n",
    "ERP_MIN_ORDER = 1000\n",
    "\n",
    "print(\"✅ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a9689",
   "metadata": {},
   "source": [
    "## Fixtures: Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6364eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope='module')\n",
    "def db_connection():\n",
    "    \"\"\"Connection to sql_retail_analytics_warehouse database.\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.autocommit = True\n",
    "    yield conn\n",
    "    conn.close()\n",
    "\n",
    "@pytest.fixture(scope='module')\n",
    "def db_cursor(db_connection):\n",
    "    \"\"\"Cursor for warehouse database.\"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    yield cursor\n",
    "    cursor.close()\n",
    "\n",
    "print(\"✅ Fixtures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e24650",
   "metadata": {},
   "source": [
    "## Test Suite 1: Job Registration Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_jobs_table_exists(db_cursor):\n",
    "    \"\"\"Verify bronze.load_jobs table exists.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'bronze'\n",
    "        AND table_name = 'load_jobs'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"bronze.load_jobs table must exist\"\n",
    "\n",
    "def test_jobs_were_registered(db_cursor):\n",
    "    \"\"\"Verify at least one job was registered.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM bronze.load_jobs\n",
    "    \"\"\")\n",
    "    \n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count > 0, \"At least one job should be registered\"\n",
    "\n",
    "def test_all_bronze_tables_have_jobs(db_cursor):\n",
    "    \"\"\"Verify all bronze data tables have corresponding job records.\"\"\"\n",
    "    # Get all bronze tables (excluding system tables)\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'bronze'\n",
    "        AND table_type = 'BASE TABLE'\n",
    "        AND table_name NOT IN ('load_jobs', 'load_log')\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "    \n",
    "    bronze_tables = [f\"bronze.{row[0]}\" for row in db_cursor.fetchall()]\n",
    "    \n",
    "    # Get all registered jobs\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM bronze.load_jobs\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "    \n",
    "    registered_jobs = [row[0] for row in db_cursor.fetchall()]\n",
    "    \n",
    "    # Every bronze table should have a job\n",
    "    missing_jobs = set(bronze_tables) - set(registered_jobs)\n",
    "    assert len(missing_jobs) == 0, \\\n",
    "        f\"These bronze tables are missing job records: {missing_jobs}\"\n",
    "\n",
    "def test_no_extra_jobs_for_nonexistent_tables(db_cursor):\n",
    "    \"\"\"Verify no job records exist for tables that don't exist.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT lj.table_name\n",
    "        FROM bronze.load_jobs lj\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM information_schema.tables t\n",
    "            WHERE format('%I.%I', t.table_schema, t.table_name) = lj.table_name\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    orphaned_jobs = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert len(orphaned_jobs) == 0, \\\n",
    "        f\"These jobs reference non-existent tables: {orphaned_jobs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972bf80",
   "metadata": {},
   "source": [
    "## Test Suite 2: File Path Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccaff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_all_enabled_jobs_have_file_paths(db_cursor):\n",
    "    \"\"\"Verify all enabled jobs have non-empty file paths.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE is_enabled = TRUE\n",
    "        AND (file_path IS NULL OR TRIM(file_path) = '')\n",
    "    \"\"\")\n",
    "    \n",
    "    missing_paths = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert len(missing_paths) == 0, \\\n",
    "        f\"These enabled jobs have missing file paths: {missing_paths}\"\n",
    "\n",
    "def test_file_paths_end_with_csv(db_cursor):\n",
    "    \"\"\"Verify all file paths end with .csv extension.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE file_path IS NOT NULL\n",
    "        AND NOT file_path ILIKE '%.csv'\n",
    "    \"\"\")\n",
    "    \n",
    "    invalid_extensions = db_cursor.fetchall()\n",
    "    assert len(invalid_extensions) == 0, \\\n",
    "        f\"These jobs have invalid file extensions: {invalid_extensions}\"\n",
    "\n",
    "def test_file_paths_no_trailing_slashes_before_filename(db_cursor):\n",
    "    \"\"\"Verify file paths don't have double slashes (//filename.csv).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE file_path LIKE '%//%'\n",
    "    \"\"\")\n",
    "    \n",
    "    double_slash_paths = db_cursor.fetchall()\n",
    "    assert len(double_slash_paths) == 0, \\\n",
    "        f\"These jobs have double slashes in paths: {double_slash_paths}\"\n",
    "\n",
    "def test_file_paths_use_correct_base_paths(db_cursor):\n",
    "    \"\"\"Verify file paths start with correct base paths from etl_config.\"\"\"\n",
    "    # Get base paths from config\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT\n",
    "            MAX(CASE WHEN config_key = 'base_path_crm' THEN config_value END) AS base_crm,\n",
    "            MAX(CASE WHEN config_key = 'base_path_erp' THEN config_value END) AS base_erp\n",
    "        FROM public.etl_config\n",
    "    \"\"\")\n",
    "    \n",
    "    base_crm, base_erp = db_cursor.fetchone()\n",
    "    \n",
    "    # Verify CRM jobs use CRM base path\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.crm_%'\n",
    "        AND NOT file_path LIKE %s\n",
    "    \"\"\", (f\"{base_crm}%\",))\n",
    "    \n",
    "    wrong_crm_paths = db_cursor.fetchall()\n",
    "    assert len(wrong_crm_paths) == 0, \\\n",
    "        f\"CRM jobs should use base path '{base_crm}': {wrong_crm_paths}\"\n",
    "    \n",
    "    # Verify ERP jobs use ERP base path\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.erp_%'\n",
    "        AND NOT file_path LIKE %s\n",
    "    \"\"\", (f\"{base_erp}%\",))\n",
    "    \n",
    "    wrong_erp_paths = db_cursor.fetchall()\n",
    "    assert len(wrong_erp_paths) == 0, \\\n",
    "        f\"ERP jobs should use base path '{base_erp}': {wrong_erp_paths}\"\n",
    "\n",
    "def test_file_paths_match_table_naming_convention(db_cursor):\n",
    "    \"\"\"Verify file paths follow source_dataset.csv → dataset.csv mapping.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path\n",
    "        FROM bronze.load_jobs\n",
    "        ORDER BY table_name\n",
    "    \"\"\")\n",
    "    \n",
    "    for table_name, file_path in db_cursor.fetchall():\n",
    "        # Extract dataset from table name (bronze.source_dataset)\n",
    "        match = re.match(r'bronze\\.(crm|erp)_(.*)', table_name)\n",
    "        if match:\n",
    "            source, dataset = match.groups()\n",
    "            expected_filename = f\"{dataset}.csv\"\n",
    "            \n",
    "            # File path should end with dataset.csv\n",
    "            assert file_path.endswith(expected_filename), \\\n",
    "                f\"Table '{table_name}' should map to file ending with '{expected_filename}', got: {file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac4f91",
   "metadata": {},
   "source": [
    "## Test Suite 3: Load Order Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_load_order_exists_for_all_jobs(db_cursor):\n",
    "    \"\"\"Verify all jobs have a load_order assigned.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE load_order IS NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    missing_order = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert len(missing_order) == 0, \\\n",
    "        f\"These jobs are missing load_order: {missing_order}\"\n",
    "\n",
    "def test_crm_tables_have_low_load_order(db_cursor):\n",
    "    \"\"\"Verify CRM tables have load_order 0-999.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, load_order\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.crm_%'\n",
    "        AND (load_order < 0 OR load_order > %s)\n",
    "    \"\"\", (CRM_MAX_ORDER,))\n",
    "    \n",
    "    invalid_crm_order = db_cursor.fetchall()\n",
    "    assert len(invalid_crm_order) == 0, \\\n",
    "        f\"CRM tables should have load_order 0-{CRM_MAX_ORDER}: {invalid_crm_order}\"\n",
    "\n",
    "def test_erp_tables_have_high_load_order(db_cursor):\n",
    "    \"\"\"Verify ERP tables have load_order 1000+.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, load_order\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.erp_%'\n",
    "        AND load_order < %s\n",
    "    \"\"\", (ERP_MIN_ORDER,))\n",
    "    \n",
    "    invalid_erp_order = db_cursor.fetchall()\n",
    "    assert len(invalid_erp_order) == 0, \\\n",
    "        f\"ERP tables should have load_order >= {ERP_MIN_ORDER}: {invalid_erp_order}\"\n",
    "\n",
    "def test_crm_loads_before_erp(db_cursor):\n",
    "    \"\"\"Verify all CRM jobs have lower load_order than all ERP jobs.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT MAX(load_order) AS max_crm_order\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.crm_%'\n",
    "    \"\"\")\n",
    "    \n",
    "    max_crm_order = db_cursor.fetchone()[0]\n",
    "    \n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT MIN(load_order) AS min_erp_order\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name LIKE 'bronze.erp_%'\n",
    "    \"\"\")\n",
    "    \n",
    "    min_erp_order = db_cursor.fetchone()[0]\n",
    "    \n",
    "    if max_crm_order is not None and min_erp_order is not None:\n",
    "        assert max_crm_order < min_erp_order, \\\n",
    "            f\"All CRM jobs (max={max_crm_order}) should load before ERP jobs (min={min_erp_order})\"\n",
    "\n",
    "def test_no_duplicate_load_orders(db_cursor):\n",
    "    \"\"\"Verify each load_order value is unique (no ties).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT load_order, COUNT(*) as count\n",
    "        FROM bronze.load_jobs\n",
    "        GROUP BY load_order\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    duplicates = db_cursor.fetchall()\n",
    "    assert len(duplicates) == 0, \\\n",
    "        f\"These load_order values are duplicated: {duplicates}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad00d2",
   "metadata": {},
   "source": [
    "## Test Suite 4: Table Naming Convention Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_all_jobs_follow_naming_pattern(db_cursor):\n",
    "    \"\"\"Verify all job table names follow bronze.source_dataset pattern.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM bronze.load_jobs\n",
    "    \"\"\")\n",
    "    \n",
    "    for (table_name,) in db_cursor.fetchall():\n",
    "        # Should match bronze.{source}_{dataset}\n",
    "        assert re.match(r'^bronze\\.(crm|erp)_.+$', table_name), \\\n",
    "            f\"Table name '{table_name}' doesn't follow bronze.source_dataset pattern\"\n",
    "\n",
    "def test_table_names_have_schema_qualifier(db_cursor):\n",
    "    \"\"\"Verify all table names are schema-qualified (bronze.tablename).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE table_name NOT LIKE 'bronze.%'\n",
    "    \"\"\")\n",
    "    \n",
    "    unqualified = [row[0] for row in db_cursor.fetchall()]\n",
    "    assert len(unqualified) == 0, \\\n",
    "        f\"These table names are not schema-qualified: {unqualified}\"\n",
    "\n",
    "def test_only_expected_source_systems(db_cursor):\n",
    "    \"\"\"Verify only CRM and ERP source systems are registered.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            SPLIT_PART(REPLACE(table_name, 'bronze.', ''), '_', 1) AS source\n",
    "        FROM bronze.load_jobs\n",
    "        ORDER BY source\n",
    "    \"\"\")\n",
    "    \n",
    "    sources = [row[0] for row in db_cursor.fetchall()]\n",
    "    unexpected = set(sources) - set(EXPECTED_SOURCES)\n",
    "    \n",
    "    assert len(unexpected) == 0, \\\n",
    "        f\"Unexpected source systems found: {unexpected}. Expected: {EXPECTED_SOURCES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4f0da",
   "metadata": {},
   "source": [
    "## Test Suite 5: Configuration Dependency Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1da255",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_etl_config_table_exists(db_cursor):\n",
    "    \"\"\"Verify public.etl_config table exists (required dependency).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public'\n",
    "        AND table_name = 'etl_config'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count == 1, \"public.etl_config table must exist\"\n",
    "\n",
    "def test_base_path_crm_configured(db_cursor):\n",
    "    \"\"\"Verify base_path_crm is configured in etl_config.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key = 'base_path_crm'\n",
    "    \"\"\")\n",
    "    \n",
    "    result = db_cursor.fetchone()\n",
    "    assert result is not None, \"base_path_crm must be configured in etl_config\"\n",
    "    assert result[0] is not None, \"base_path_crm value cannot be NULL\"\n",
    "    assert len(result[0].strip()) > 0, \"base_path_crm cannot be empty\"\n",
    "\n",
    "def test_base_path_erp_configured(db_cursor):\n",
    "    \"\"\"Verify base_path_erp is configured in etl_config.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key = 'base_path_erp'\n",
    "    \"\"\")\n",
    "    \n",
    "    result = db_cursor.fetchone()\n",
    "    assert result is not None, \"base_path_erp must be configured in etl_config\"\n",
    "    assert result[0] is not None, \"base_path_erp value cannot be NULL\"\n",
    "    assert len(result[0].strip()) > 0, \"base_path_erp cannot be empty\"\n",
    "\n",
    "def test_base_paths_no_trailing_slashes(db_cursor):\n",
    "    \"\"\"Verify base paths in etl_config don't have trailing slashes (convention).\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT config_key, config_value\n",
    "        FROM public.etl_config\n",
    "        WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "        AND config_value LIKE '%/'\n",
    "    \"\"\")\n",
    "    \n",
    "    trailing_slashes = db_cursor.fetchall()\n",
    "    assert len(trailing_slashes) == 0, \\\n",
    "        f\"Base paths should NOT have trailing slashes: {trailing_slashes}\"\n",
    "\n",
    "def test_seed_load_jobs_procedure_exists(db_cursor):\n",
    "    \"\"\"Verify setup.seed_load_jobs() procedure exists.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM pg_proc p\n",
    "        JOIN pg_namespace n ON p.pronamespace = n.oid\n",
    "        WHERE n.nspname = 'setup'\n",
    "        AND p.proname = 'seed_load_jobs'\n",
    "    \"\"\")\n",
    "    \n",
    "    count = db_cursor.fetchone()[0]\n",
    "    assert count > 0, \"setup.seed_load_jobs() procedure must exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c1f5b8",
   "metadata": {},
   "source": [
    "## Test Suite 6: Job Metadata Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_no_duplicate_table_names(db_cursor):\n",
    "    \"\"\"Verify no duplicate table entries in load_jobs.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, COUNT(*) as count\n",
    "        FROM bronze.load_jobs\n",
    "        GROUP BY table_name\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    duplicates = db_cursor.fetchall()\n",
    "    assert len(duplicates) == 0, \\\n",
    "        f\"Duplicate table entries found: {duplicates}\"\n",
    "\n",
    "def test_is_enabled_is_boolean(db_cursor):\n",
    "    \"\"\"Verify is_enabled column contains only boolean values.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, is_enabled\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE is_enabled IS NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    null_enabled = db_cursor.fetchall()\n",
    "    assert len(null_enabled) == 0, \\\n",
    "        f\"These jobs have NULL is_enabled values: {null_enabled}\"\n",
    "\n",
    "def test_at_least_one_job_enabled(db_cursor):\n",
    "    \"\"\"Verify at least one job is enabled for loading.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM bronze.load_jobs\n",
    "        WHERE is_enabled = TRUE\n",
    "    \"\"\")\n",
    "    \n",
    "    enabled_count = db_cursor.fetchone()[0]\n",
    "    assert enabled_count > 0, \"At least one job should be enabled\"\n",
    "\n",
    "def test_table_name_is_primary_key(db_cursor):\n",
    "    \"\"\"Verify table_name column is the primary key.\"\"\"\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.table_constraints\n",
    "        WHERE table_schema = 'bronze'\n",
    "        AND table_name = 'load_jobs'\n",
    "        AND constraint_type = 'PRIMARY KEY'\n",
    "    \"\"\")\n",
    "    \n",
    "    pk_count = db_cursor.fetchone()[0]\n",
    "    assert pk_count == 1, \"bronze.load_jobs should have a primary key\"\n",
    "    \n",
    "    # Verify it's on table_name column\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM information_schema.key_column_usage\n",
    "        WHERE table_schema = 'bronze'\n",
    "        AND table_name = 'load_jobs'\n",
    "        AND constraint_name IN (\n",
    "            SELECT constraint_name\n",
    "            FROM information_schema.table_constraints\n",
    "            WHERE table_schema = 'bronze'\n",
    "            AND table_name = 'load_jobs'\n",
    "            AND constraint_type = 'PRIMARY KEY'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    pk_column = db_cursor.fetchone()[0]\n",
    "    assert pk_column == 'table_name', \\\n",
    "        f\"Primary key should be on table_name column, got: {pk_column}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f370197",
   "metadata": {},
   "source": [
    "## Test Suite 7: Idempotency Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16342432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "def test_re_registration_is_safe(db_cursor):\n",
    "    \"\"\"Verify calling setup.seed_load_jobs() multiple times doesn't create duplicates.\"\"\"\n",
    "    # Count jobs before\n",
    "    db_cursor.execute(\"SELECT COUNT(*) FROM bronze.load_jobs\")\n",
    "    count_before = db_cursor.fetchone()[0]\n",
    "    \n",
    "    # Re-run the seeder\n",
    "    db_cursor.execute(\"CALL setup.seed_load_jobs()\")\n",
    "    \n",
    "    # Count jobs after\n",
    "    db_cursor.execute(\"SELECT COUNT(*) FROM bronze.load_jobs\")\n",
    "    count_after = db_cursor.fetchone()[0]\n",
    "    \n",
    "    assert count_before == count_after, \\\n",
    "        f\"Job count changed after re-registration: {count_before} → {count_after}\"\n",
    "\n",
    "def test_upsert_updates_existing_records(db_cursor):\n",
    "    \"\"\"Verify re-registration updates existing records (doesn't just ignore).\"\"\"\n",
    "    # Get a sample job\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path, load_order\n",
    "        FROM bronze.load_jobs\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    \n",
    "    result = db_cursor.fetchone()\n",
    "    if result:\n",
    "        table_name, original_path, original_order = result\n",
    "        \n",
    "        # Temporarily modify the record\n",
    "        db_cursor.execute(\"\"\"\n",
    "            UPDATE bronze.load_jobs\n",
    "            SET file_path = 'TEMP_TEST_PATH.csv'\n",
    "            WHERE table_name = %s\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        # Re-run seeder (should restore correct path)\n",
    "        db_cursor.execute(\"CALL setup.seed_load_jobs()\")\n",
    "        \n",
    "        # Verify it was updated back\n",
    "        db_cursor.execute(\"\"\"\n",
    "            SELECT file_path\n",
    "            FROM bronze.load_jobs\n",
    "            WHERE table_name = %s\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        restored_path = db_cursor.fetchone()[0]\n",
    "        assert restored_path != 'TEMP_TEST_PATH.csv', \\\n",
    "            \"Re-registration should update existing records\"\n",
    "        assert restored_path == original_path, \\\n",
    "            f\"Path should be restored to original: {original_path}\"\n",
    "\n",
    "def test_primary_key_prevents_duplicates(db_cursor):\n",
    "    \"\"\"Verify primary key constraint prevents duplicate table entries.\"\"\"\n",
    "    # Get a sample job\n",
    "    db_cursor.execute(\"\"\"\n",
    "        SELECT table_name, file_path, is_enabled, load_order\n",
    "        FROM bronze.load_jobs\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    \n",
    "    result = db_cursor.fetchone()\n",
    "    if result:\n",
    "        table_name, file_path, is_enabled, load_order = result\n",
    "        \n",
    "        # Try to insert duplicate (should fail)\n",
    "        try:\n",
    "            db_cursor.execute(\"\"\"\n",
    "                INSERT INTO bronze.load_jobs (table_name, file_path, is_enabled, load_order)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "            \"\"\", (table_name, file_path, is_enabled, load_order))\n",
    "            \n",
    "            # If we get here, the constraint didn't work\n",
    "            assert False, \"Primary key should prevent duplicate inserts\"\n",
    "            \n",
    "        except psycopg2.errors.UniqueViolation:\n",
    "            # This is expected - rollback and pass\n",
    "            db_cursor.connection.rollback()\n",
    "            db_cursor.connection.autocommit = True\n",
    "            assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43a8c9",
   "metadata": {},
   "source": [
    "## Summary: Run All Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests in this notebook\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327722b",
   "metadata": {},
   "source": [
    "## Manual Inspection: Job Registry Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to warehouse database\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "# Get comprehensive job registry information\n",
    "df_jobs = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        table_name,\n",
    "        file_path,\n",
    "        is_enabled,\n",
    "        load_order,\n",
    "        CASE\n",
    "            WHEN load_order < 1000 THEN 'CRM'\n",
    "            ELSE 'ERP'\n",
    "        END AS source_system,\n",
    "        SPLIT_PART(file_path, '/', -1) AS filename\n",
    "    FROM bronze.load_jobs\n",
    "    ORDER BY load_order\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n📋 Registered Bronze Load Jobs:\")\n",
    "display(df_jobs)\n",
    "\n",
    "# Get job count by source system\n",
    "df_job_counts = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        CASE\n",
    "            WHEN table_name LIKE 'bronze.crm_%' THEN 'CRM'\n",
    "            WHEN table_name LIKE 'bronze.erp_%' THEN 'ERP'\n",
    "            ELSE 'Other'\n",
    "        END AS source_system,\n",
    "        COUNT(*) AS job_count,\n",
    "        SUM(CASE WHEN is_enabled THEN 1 ELSE 0 END) AS enabled_count,\n",
    "        MIN(load_order) AS min_load_order,\n",
    "        MAX(load_order) AS max_load_order\n",
    "    FROM bronze.load_jobs\n",
    "    GROUP BY source_system\n",
    "    ORDER BY min_load_order\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n📊 Job Statistics by Source System:\")\n",
    "display(df_job_counts)\n",
    "\n",
    "# Get configuration values used\n",
    "df_config = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        config_key,\n",
    "        config_value,\n",
    "        CASE\n",
    "            WHEN config_value LIKE '%/' THEN '⚠️  Has trailing slash'\n",
    "            ELSE '✅ Correct format'\n",
    "        END AS validation\n",
    "    FROM public.etl_config\n",
    "    WHERE config_key IN ('base_path_crm', 'base_path_erp')\n",
    "    ORDER BY config_key\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n⚙️  Configuration Used for Job Registration:\")\n",
    "display(df_config)\n",
    "\n",
    "# Get file path validation results\n",
    "df_path_validation = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        table_name,\n",
    "        file_path,\n",
    "        CASE\n",
    "            WHEN file_path IS NULL THEN '❌ NULL path'\n",
    "            WHEN file_path LIKE '%//%' THEN '❌ Double slashes'\n",
    "            WHEN NOT file_path LIKE '%.csv' THEN '❌ Wrong extension'\n",
    "            ELSE '✅ Valid'\n",
    "        END AS path_status\n",
    "    FROM bronze.load_jobs\n",
    "    ORDER BY\n",
    "        CASE\n",
    "            WHEN file_path IS NULL THEN 1\n",
    "            WHEN file_path LIKE '%//%' THEN 2\n",
    "            WHEN NOT file_path LIKE '%.csv' THEN 3\n",
    "            ELSE 4\n",
    "        END,\n",
    "        table_name\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n🔍 File Path Validation:\")\n",
    "display(df_path_validation)\n",
    "\n",
    "# Check for any tables without jobs\n",
    "df_missing_jobs = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        format('bronze.%I', t.table_name) AS table_name,\n",
    "        '⚠️  No job registered' AS status\n",
    "    FROM information_schema.tables t\n",
    "    WHERE t.table_schema = 'bronze'\n",
    "    AND t.table_type = 'BASE TABLE'\n",
    "    AND t.table_name NOT IN ('load_jobs', 'load_log')\n",
    "    AND NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM bronze.load_jobs lj\n",
    "        WHERE lj.table_name = format('bronze.%I', t.table_name)\n",
    "    )\n",
    "    ORDER BY table_name\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n⚠️  Tables Missing Job Registration:\")\n",
    "if len(df_missing_jobs) > 0:\n",
    "    display(df_missing_jobs)\n",
    "else:\n",
    "    print(\"   ✅ All bronze tables have job registrations\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\n✅ Inspection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854dd90",
   "metadata": {},
   "source": [
    "## Load Order Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of load order\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "df_order = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        load_order,\n",
    "        table_name,\n",
    "        SPLIT_PART(file_path, '/', -1) AS file,\n",
    "        is_enabled,\n",
    "        CASE\n",
    "            WHEN load_order < 1000 THEN '🟦 CRM'\n",
    "            ELSE '🟨 ERP'\n",
    "        END AS source\n",
    "    FROM bronze.load_jobs\n",
    "    ORDER BY load_order\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\n📈 Load Execution Sequence:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df_order.iterrows():\n",
    "    status = \"✅\" if row['is_enabled'] else \"⏸️\"\n",
    "    print(f\"{status} [{row['load_order']:4d}] {row['source']} | {row['table_name']:30s} ← {row['file']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}